{
    "sourceFile": "node_modules/next/dist/esm/client/components/segment-cache-impl/scheduler.js",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1746892784688,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1746891704042,
            "name": "cart",
            "content": "import { matchSegment } from '../match-segments';\nimport { readOrCreateRouteCacheEntry, readOrCreateSegmentCacheEntry, fetchRouteOnCacheMiss, fetchSegmentOnCacheMiss, EntryStatus, fetchSegmentPrefetchesUsingDynamicRequest, convertRouteTreeToFlightRouterState, FetchStrategy, readOrCreateRevalidatingSegmentEntry, upsertSegmentEntry, upgradeToPendingSegment, waitForSegmentCacheEntry, resetRevalidatingSegmentEntry, getSegmentKeypathForTask } from './cache';\nimport { PrefetchPriority } from '../segment-cache';\nconst scheduleMicrotask = typeof queueMicrotask === 'function' ? queueMicrotask : (fn)=>Promise.resolve().then(fn).catch((error)=>setTimeout(()=>{\n            throw error;\n        }));\n;\n;\nconst taskHeap = [];\n// This is intentionally low so that when a navigation happens, the browser's\n// internal network queue is not already saturated with prefetch requests.\nconst MAX_CONCURRENT_PREFETCH_REQUESTS = 3;\nlet inProgressRequests = 0;\nlet sortIdCounter = 0;\nlet didScheduleMicrotask = false;\n/**\n * Initiates a prefetch task for the given URL. If a prefetch for the same URL\n * is already in progress, this will bump it to the top of the queue.\n *\n * This is not a user-facing function. By the time this is called, the href is\n * expected to be validated and normalized.\n *\n * @param key The RouteCacheKey to prefetch.\n * @param treeAtTimeOfPrefetch The app's current FlightRouterState\n * @param includeDynamicData Whether to prefetch dynamic data, in addition to\n * static data. This is used by <Link prefetch={true}>.\n */ export function schedulePrefetchTask(key, treeAtTimeOfPrefetch, includeDynamicData, priority) {\n    // Spawn a new prefetch task\n    const task = {\n        key,\n        treeAtTimeOfPrefetch,\n        priority,\n        phase: 1,\n        hasBackgroundWork: false,\n        includeDynamicData,\n        sortId: sortIdCounter++,\n        isCanceled: false,\n        _heapIndex: -1\n    };\n    heapPush(taskHeap, task);\n    // Schedule an async task to process the queue.\n    //\n    // The main reason we process the queue in an async task is for batching.\n    // It's common for a single JS task/event to trigger multiple prefetches.\n    // By deferring to a microtask, we only process the queue once per JS task.\n    // If they have different priorities, it also ensures they are processed in\n    // the optimal order.\n    ensureWorkIsScheduled();\n    return task;\n}\nexport function cancelPrefetchTask(task) {\n    // Remove the prefetch task from the queue. If the task already completed,\n    // then this is a no-op.\n    //\n    // We must also explicitly mark the task as canceled so that a blocked task\n    // does not get added back to the queue when it's pinged by the network.\n    task.isCanceled = true;\n    heapDelete(taskHeap, task);\n}\nexport function reschedulePrefetchTask(task, treeAtTimeOfPrefetch, includeDynamicData, priority) {\n    // Bump the prefetch task to the top of the queue, as if it were a fresh\n    // task. This is essentially the same as canceling the task and scheduling\n    // a new one, except it reuses the original object.\n    //\n    // The primary use case is to increase the priority of a Link-initated\n    // prefetch on hover.\n    // Un-cancel the task, in case it was previously canceled.\n    task.isCanceled = false;\n    task.phase = 1;\n    // Assign a new sort ID to move it ahead of all other tasks at the same\n    // priority level. (Higher sort IDs are processed first.)\n    task.sortId = sortIdCounter++;\n    task.priority = priority;\n    task.treeAtTimeOfPrefetch = treeAtTimeOfPrefetch;\n    task.includeDynamicData = includeDynamicData;\n    if (task._heapIndex !== -1) {\n        // The task is already in the queue.\n        heapResift(taskHeap, task);\n    } else {\n        heapPush(taskHeap, task);\n    }\n    ensureWorkIsScheduled();\n}\nfunction ensureWorkIsScheduled() {\n    if (didScheduleMicrotask || !hasNetworkBandwidth()) {\n        // Either we already scheduled a task to process the queue, or there are\n        // too many concurrent requests in progress. In the latter case, the\n        // queue will resume processing once more bandwidth is available.\n        return;\n    }\n    didScheduleMicrotask = true;\n    scheduleMicrotask(processQueueInMicrotask);\n}\n/**\n * Checks if we've exceeded the maximum number of concurrent prefetch requests,\n * to avoid saturating the browser's internal network queue. This is a\n * cooperative limit — prefetch tasks should check this before issuing\n * new requests.\n */ function hasNetworkBandwidth() {\n    // TODO: Also check if there's an in-progress navigation. We should never\n    // add prefetch requests to the network queue if an actual navigation is\n    // taking place, to ensure there's sufficient bandwidth for render-blocking\n    // data and resources.\n    return inProgressRequests < MAX_CONCURRENT_PREFETCH_REQUESTS;\n}\nfunction spawnPrefetchSubtask(prefetchSubtask) {\n    // When the scheduler spawns an async task, we don't await its result.\n    // Instead, the async task writes its result directly into the cache, then\n    // pings the scheduler to continue.\n    //\n    // We process server responses streamingly, so the prefetch subtask will\n    // likely resolve before we're finished receiving all the data. The subtask\n    // result includes a promise that resolves once the network connection is\n    // closed. The scheduler uses this to control network bandwidth by tracking\n    // and limiting the number of concurrent requests.\n    inProgressRequests++;\n    return prefetchSubtask.then((result)=>{\n        if (result === null) {\n            // The prefetch task errored before it could start processing the\n            // network stream. Assume the connection is closed.\n            onPrefetchConnectionClosed();\n            return null;\n        }\n        // Wait for the connection to close before freeing up more bandwidth.\n        result.closed.then(onPrefetchConnectionClosed);\n        return result.value;\n    });\n}\nfunction onPrefetchConnectionClosed() {\n    inProgressRequests--;\n    // Notify the scheduler that we have more bandwidth, and can continue\n    // processing tasks.\n    ensureWorkIsScheduled();\n}\n/**\n * Notify the scheduler that we've received new data for an in-progress\n * prefetch. The corresponding task will be added back to the queue (unless the\n * task has been canceled in the meantime).\n */ export function pingPrefetchTask(task) {\n    // \"Ping\" a prefetch that's already in progress to notify it of new data.\n    if (// Check if prefetch was canceled.\n    task.isCanceled || // Check if prefetch is already queued.\n    task._heapIndex !== -1) {\n        return;\n    }\n    // Add the task back to the queue.\n    heapPush(taskHeap, task);\n    ensureWorkIsScheduled();\n}\nfunction processQueueInMicrotask() {\n    didScheduleMicrotask = false;\n    // We aim to minimize how often we read the current time. Since nearly all\n    // functions in the prefetch scheduler are synchronous, we can read the time\n    // once and pass it as an argument wherever it's needed.\n    const now = Date.now();\n    // Process the task queue until we run out of network bandwidth.\n    let task = heapPeek(taskHeap);\n    while(task !== null && hasNetworkBandwidth()){\n        const route = readOrCreateRouteCacheEntry(now, task);\n        const exitStatus = pingRootRouteTree(now, task, route);\n        // The `hasBackgroundWork` field is only valid for a single attempt. Reset\n        // it immediately upon exit.\n        const hasBackgroundWork = task.hasBackgroundWork;\n        task.hasBackgroundWork = false;\n        switch(exitStatus){\n            case 0:\n                // The task yielded because there are too many requests in progress.\n                // Stop processing tasks until we have more bandwidth.\n                return;\n            case 1:\n                // The task is blocked. It needs more data before it can proceed.\n                // Keep the task out of the queue until the server responds.\n                heapPop(taskHeap);\n                // Continue to the next task\n                task = heapPeek(taskHeap);\n                continue;\n            case 2:\n                if (task.phase === 1) {\n                    // Finished prefetching the route tree. Proceed to prefetching\n                    // the segments.\n                    task.phase = 0;\n                    heapResift(taskHeap, task);\n                } else if (hasBackgroundWork) {\n                    // The task spawned additional background work. Reschedule the task\n                    // at background priority.\n                    task.priority = PrefetchPriority.Background;\n                    heapResift(taskHeap, task);\n                } else {\n                    // The prefetch is complete. Continue to the next task.\n                    heapPop(taskHeap);\n                }\n                task = heapPeek(taskHeap);\n                continue;\n            default:\n                exitStatus;\n        }\n    }\n}\n/**\n * Check this during a prefetch task to determine if background work can be\n * performed. If so, it evaluates to `true`. Otherwise, it returns `false`,\n * while also scheduling a background task to run later. Usage:\n *\n * @example\n * if (background(task)) {\n *   // Perform background-pri work\n * }\n */ function background(task) {\n    if (task.priority === PrefetchPriority.Background) {\n        return true;\n    }\n    task.hasBackgroundWork = true;\n    return false;\n}\nfunction pingRootRouteTree(now, task, route) {\n    switch(route.status){\n        case EntryStatus.Empty:\n            {\n                // Route is not yet cached, and there's no request already in progress.\n                // Spawn a task to request the route, load it into the cache, and ping\n                // the task to continue.\n                // TODO: There are multiple strategies in the <Link> API for prefetching\n                // a route. Currently we've only implemented the main one: per-segment,\n                // static-data only.\n                //\n                // There's also <Link prefetch={true}> which prefetches both static *and*\n                // dynamic data. Similarly, we need to fallback to the old, per-page\n                // behavior if PPR is disabled for a route (via the incremental opt-in).\n                //\n                // Those cases will be handled here.\n                spawnPrefetchSubtask(fetchRouteOnCacheMiss(route, task));\n                // If the request takes longer than a minute, a subsequent request should\n                // retry instead of waiting for this one. When the response is received,\n                // this value will be replaced by a new value based on the stale time sent\n                // from the server.\n                // TODO: We should probably also manually abort the fetch task, to reclaim\n                // server bandwidth.\n                route.staleAt = now + 60 * 1000;\n                // Upgrade to Pending so we know there's already a request in progress\n                route.status = EntryStatus.Pending;\n            // Intentional fallthrough to the Pending branch\n            }\n        case EntryStatus.Pending:\n            {\n                // Still pending. We can't start prefetching the segments until the route\n                // tree has loaded. Add the task to the set of blocked tasks so that it\n                // is notified when the route tree is ready.\n                const blockedTasks = route.blockedTasks;\n                if (blockedTasks === null) {\n                    route.blockedTasks = new Set([\n                        task\n                    ]);\n                } else {\n                    blockedTasks.add(task);\n                }\n                return 1;\n            }\n        case EntryStatus.Rejected:\n            {\n                // Route tree failed to load. Treat as a 404.\n                return 2;\n            }\n        case EntryStatus.Fulfilled:\n            {\n                if (task.phase !== 0) {\n                    // Do not prefetch segment data until we've entered the segment phase.\n                    return 2;\n                }\n                // Recursively fill in the segment tree.\n                if (!hasNetworkBandwidth()) {\n                    // Stop prefetching segments until there's more bandwidth.\n                    return 0;\n                }\n                const tree = route.tree;\n                // Determine which fetch strategy to use for this prefetch task.\n                const fetchStrategy = task.includeDynamicData ? FetchStrategy.Full : route.isPPREnabled ? FetchStrategy.PPR : FetchStrategy.LoadingBoundary;\n                switch(fetchStrategy){\n                    case FetchStrategy.PPR:\n                        // Individually prefetch the static shell for each segment. This is\n                        // the default prefetching behavior for static routes, or when PPR is\n                        // enabled. It will not include any dynamic data.\n                        return pingPPRRouteTree(now, task, route, tree);\n                    case FetchStrategy.Full:\n                    case FetchStrategy.LoadingBoundary:\n                        {\n                            // Prefetch multiple segments using a single dynamic request.\n                            const spawnedEntries = new Map();\n                            const dynamicRequestTree = diffRouteTreeAgainstCurrent(now, task, route, task.treeAtTimeOfPrefetch, tree, spawnedEntries, fetchStrategy);\n                            const needsDynamicRequest = spawnedEntries.size > 0;\n                            if (needsDynamicRequest) {\n                                // Perform a dynamic prefetch request and populate the cache with\n                                // the result\n                                spawnPrefetchSubtask(fetchSegmentPrefetchesUsingDynamicRequest(task, route, fetchStrategy, dynamicRequestTree, spawnedEntries));\n                            }\n                            return 2;\n                        }\n                    default:\n                        fetchStrategy;\n                }\n                break;\n            }\n        default:\n            {\n                route;\n            }\n    }\n    return 2;\n}\nfunction pingPPRRouteTree(now, task, route, tree) {\n    const segment = readOrCreateSegmentCacheEntry(now, task, route, tree.key);\n    pingPerSegment(now, task, route, segment, task.key, tree.key);\n    if (tree.slots !== null) {\n        if (!hasNetworkBandwidth()) {\n            // Stop prefetching segments until there's more bandwidth.\n            return 0;\n        }\n        // Recursively ping the children.\n        for(const parallelRouteKey in tree.slots){\n            const childTree = tree.slots[parallelRouteKey];\n            const childExitStatus = pingPPRRouteTree(now, task, route, childTree);\n            if (childExitStatus === 0) {\n                // Child yielded without finishing.\n                return 0;\n            }\n        }\n    }\n    // This segment and all its children have finished prefetching.\n    return 2;\n}\nfunction diffRouteTreeAgainstCurrent(now, task, route, oldTree, newTree, spawnedEntries, fetchStrategy) {\n    // This is a single recursive traversal that does multiple things:\n    // - Finds the parts of the target route (newTree) that are not part of\n    //   of the current page (oldTree) by diffing them, using the same algorithm\n    //   as a real navigation.\n    // - Constructs a request tree (FlightRouterState) that describes which\n    //   segments need to be prefetched and which ones are already cached.\n    // - Creates a set of pending cache entries for the segments that need to\n    //   be prefetched, so that a subsequent prefetch task does not request the\n    //   same segments again.\n    const oldTreeChildren = oldTree[1];\n    const newTreeChildren = newTree.slots;\n    let requestTreeChildren = {};\n    if (newTreeChildren !== null) {\n        for(const parallelRouteKey in newTreeChildren){\n            const newTreeChild = newTreeChildren[parallelRouteKey];\n            const newTreeChildSegment = newTreeChild.segment;\n            const oldTreeChild = oldTreeChildren[parallelRouteKey];\n            const oldTreeChildSegment = oldTreeChild == null ? void 0 : oldTreeChild[0];\n            if (oldTreeChildSegment !== undefined && matchSegment(newTreeChildSegment, oldTreeChildSegment)) {\n                // This segment is already part of the current route. Keep traversing.\n                const requestTreeChild = diffRouteTreeAgainstCurrent(now, task, route, oldTreeChild, newTreeChild, spawnedEntries, fetchStrategy);\n                requestTreeChildren[parallelRouteKey] = requestTreeChild;\n            } else {\n                // This segment is not part of the current route. We're entering a\n                // part of the tree that we need to prefetch (unless everything is\n                // already cached).\n                switch(fetchStrategy){\n                    case FetchStrategy.LoadingBoundary:\n                        {\n                            // When PPR is disabled, we can't prefetch per segment. We must\n                            // fallback to the old prefetch behavior and send a dynamic request.\n                            // Only routes that include a loading boundary can be prefetched in\n                            // this way.\n                            //\n                            // This is simlar to a \"full\" prefetch, but we're much more\n                            // conservative about which segments to include in the request.\n                            //\n                            // The server will only render up to the first loading boundary\n                            // inside new part of the tree. If there's no loading boundary, the\n                            // server will never return any data. TODO: When we prefetch the\n                            // route tree, the server should indicate whether there's a loading\n                            // boundary so the client doesn't send a second request for no\n                            // reason.\n                            const requestTreeChild = pingPPRDisabledRouteTreeUpToLoadingBoundary(now, task, route, newTreeChild, null, spawnedEntries);\n                            requestTreeChildren[parallelRouteKey] = requestTreeChild;\n                            break;\n                        }\n                    case FetchStrategy.Full:\n                        {\n                            // This is a \"full\" prefetch. Fetch all the data in the tree, both\n                            // static and dynamic. We issue roughly the same request that we\n                            // would during a real navigation. The goal is that once the\n                            // navigation occurs, the router should not have to fetch any\n                            // additional data.\n                            //\n                            // Although the response will include dynamic data, opting into a\n                            // Full prefetch — via <Link prefetch={true}> — implicitly\n                            // instructs the cache to treat the response as \"static\", or non-\n                            // dynamic, since the whole point is to cache it for\n                            // future navigations.\n                            //\n                            // Construct a tree (currently a FlightRouterState) that represents\n                            // which segments need to be prefetched and which ones are already\n                            // cached. If the tree is empty, then we can exit. Otherwise, we'll\n                            // send the request tree to the server and use the response to\n                            // populate the segment cache.\n                            const requestTreeChild = pingRouteTreeAndIncludeDynamicData(now, task, route, newTreeChild, false, spawnedEntries);\n                            requestTreeChildren[parallelRouteKey] = requestTreeChild;\n                            break;\n                        }\n                    default:\n                        fetchStrategy;\n                }\n            }\n        }\n    }\n    const requestTree = [\n        newTree.segment,\n        requestTreeChildren,\n        null,\n        null,\n        newTree.isRootLayout\n    ];\n    return requestTree;\n}\nfunction pingPPRDisabledRouteTreeUpToLoadingBoundary(now, task, route, tree, refetchMarkerContext, spawnedEntries) {\n    // This function is similar to pingRouteTreeAndIncludeDynamicData, except the\n    // server is only going to return a minimal loading state — it will stop\n    // rendering at the first loading boundary. Whereas a Full prefetch is\n    // intentionally aggressive and tries to pretfetch all the data that will be\n    // needed for a navigation, a LoadingBoundary prefetch is much more\n    // conservative. For example, it will omit from the request tree any segment\n    // that is already cached, regardles of whether it's partial or full. By\n    // contrast, a Full prefetch will refetch partial segments.\n    // \"inside-shared-layout\" tells the server where to start looking for a\n    // loading boundary.\n    let refetchMarker = refetchMarkerContext === null ? 'inside-shared-layout' : null;\n    const segment = readOrCreateSegmentCacheEntry(now, task, route, tree.key);\n    switch(segment.status){\n        case EntryStatus.Empty:\n            {\n                // This segment is not cached. Add a refetch marker so the server knows\n                // to start rendering here.\n                // TODO: Instead of a \"refetch\" marker, we could just omit this subtree's\n                // FlightRouterState from the request tree. I think this would probably\n                // already work even without any updates to the server. For consistency,\n                // though, I'll send the full tree and we'll look into this later as part\n                // of a larger redesign of the request protocol.\n                // Add the pending cache entry to the result map.\n                spawnedEntries.set(tree.key, upgradeToPendingSegment(segment, // Set the fetch strategy to LoadingBoundary to indicate that the server\n                // might not include it in the pending response. If another route is able\n                // to issue a per-segment request, we'll do that in the background.\n                FetchStrategy.LoadingBoundary));\n                if (refetchMarkerContext !== 'refetch') {\n                    refetchMarker = refetchMarkerContext = 'refetch';\n                } else {\n                // There's already a parent with a refetch marker, so we don't need\n                // to add another one.\n                }\n                break;\n            }\n        case EntryStatus.Fulfilled:\n            {\n                // The segment is already cached.\n                // TODO: The server should include a `hasLoading` field as part of the\n                // route tree prefetch.\n                if (segment.loading !== null) {\n                    // This segment has a loading boundary, which means the server won't\n                    // render its children. So there's nothing left to prefetch along this\n                    // path. We can bail out.\n                    return convertRouteTreeToFlightRouterState(tree);\n                }\n                break;\n            }\n        case EntryStatus.Pending:\n            {\n                break;\n            }\n        case EntryStatus.Rejected:\n            {\n                break;\n            }\n        default:\n            segment;\n    }\n    const requestTreeChildren = {};\n    if (tree.slots !== null) {\n        for(const parallelRouteKey in tree.slots){\n            const childTree = tree.slots[parallelRouteKey];\n            requestTreeChildren[parallelRouteKey] = pingPPRDisabledRouteTreeUpToLoadingBoundary(now, task, route, childTree, refetchMarkerContext, spawnedEntries);\n        }\n    }\n    const requestTree = [\n        tree.segment,\n        requestTreeChildren,\n        null,\n        refetchMarker,\n        tree.isRootLayout\n    ];\n    return requestTree;\n}\nfunction pingRouteTreeAndIncludeDynamicData(now, task, route, tree, isInsideRefetchingParent, spawnedEntries) {\n    // The tree we're constructing is the same shape as the tree we're navigating\n    // to. But even though this is a \"new\" tree, some of the individual segments\n    // may be cached as a result of other route prefetches.\n    //\n    // So we need to find the first uncached segment along each path add an\n    // explicit \"refetch\" marker so the server knows where to start rendering.\n    // Once the server starts rendering along a path, it keeps rendering the\n    // entire subtree.\n    const segment = readOrCreateSegmentCacheEntry(now, task, route, tree.key);\n    let spawnedSegment = null;\n    switch(segment.status){\n        case EntryStatus.Empty:\n            {\n                // This segment is not cached. Include it in the request.\n                spawnedSegment = upgradeToPendingSegment(segment, FetchStrategy.Full);\n                break;\n            }\n        case EntryStatus.Fulfilled:\n            {\n                // The segment is already cached.\n                if (segment.isPartial) {\n                    // The cached segment contians dynamic holes. Since this is a Full\n                    // prefetch, we need to include it in the request.\n                    spawnedSegment = pingFullSegmentRevalidation(now, task, route, segment, tree.key);\n                }\n                break;\n            }\n        case EntryStatus.Pending:\n        case EntryStatus.Rejected:\n            {\n                // There's either another prefetch currently in progress, or the previous\n                // attempt failed. If it wasn't a Full prefetch, fetch it again.\n                if (segment.fetchStrategy !== FetchStrategy.Full) {\n                    spawnedSegment = pingFullSegmentRevalidation(now, task, route, segment, tree.key);\n                }\n                break;\n            }\n        default:\n            segment;\n    }\n    const requestTreeChildren = {};\n    if (tree.slots !== null) {\n        for(const parallelRouteKey in tree.slots){\n            const childTree = tree.slots[parallelRouteKey];\n            requestTreeChildren[parallelRouteKey] = pingRouteTreeAndIncludeDynamicData(now, task, route, childTree, isInsideRefetchingParent || spawnedSegment !== null, spawnedEntries);\n        }\n    }\n    if (spawnedSegment !== null) {\n        // Add the pending entry to the result map.\n        spawnedEntries.set(tree.key, spawnedSegment);\n    }\n    // Don't bother to add a refetch marker if one is already present in a parent.\n    const refetchMarker = !isInsideRefetchingParent && spawnedSegment !== null ? 'refetch' : null;\n    const requestTree = [\n        tree.segment,\n        requestTreeChildren,\n        null,\n        refetchMarker,\n        tree.isRootLayout\n    ];\n    return requestTree;\n}\nfunction pingPerSegment(now, task, route, segment, routeKey, segmentKey) {\n    switch(segment.status){\n        case EntryStatus.Empty:\n            // Upgrade to Pending so we know there's already a request in progress\n            spawnPrefetchSubtask(fetchSegmentOnCacheMiss(route, upgradeToPendingSegment(segment, FetchStrategy.PPR), routeKey, segmentKey));\n            break;\n        case EntryStatus.Pending:\n            {\n                // There's already a request in progress. Depending on what kind of\n                // request it is, we may want to revalidate it.\n                switch(segment.fetchStrategy){\n                    case FetchStrategy.PPR:\n                    case FetchStrategy.Full:\n                        break;\n                    case FetchStrategy.LoadingBoundary:\n                        // There's a pending request, but because it's using the old\n                        // prefetching strategy, we can't be sure if it will be fulfilled by\n                        // the response — it might be inside the loading boundary. Perform\n                        // a revalidation, but because it's speculative, wait to do it at\n                        // background priority.\n                        if (background(task)) {\n                            // TODO: Instead of speculatively revalidating, consider including\n                            // `hasLoading` in the route tree prefetch response.\n                            pingPPRSegmentRevalidation(now, task, segment, route, routeKey, segmentKey);\n                        }\n                        break;\n                    default:\n                        segment.fetchStrategy;\n                }\n                break;\n            }\n        case EntryStatus.Rejected:\n            {\n                // The existing entry in the cache was rejected. Depending on how it\n                // was originally fetched, we may or may not want to revalidate it.\n                switch(segment.fetchStrategy){\n                    case FetchStrategy.PPR:\n                    case FetchStrategy.Full:\n                        break;\n                    case FetchStrategy.LoadingBoundary:\n                        // There's a rejected entry, but it was fetched using the loading\n                        // boundary strategy. So the reason it wasn't returned by the server\n                        // might just be because it was inside a loading boundary. Or because\n                        // there was a dynamic rewrite. Revalidate it using the per-\n                        // segment strategy.\n                        //\n                        // Because a rejected segment will definitely prevent the segment (and\n                        // all of its children) from rendering, we perform this revalidation\n                        // immediately instead of deferring it to a background task.\n                        pingPPRSegmentRevalidation(now, task, segment, route, routeKey, segmentKey);\n                        break;\n                    default:\n                        segment.fetchStrategy;\n                }\n                break;\n            }\n        case EntryStatus.Fulfilled:\n            break;\n        default:\n            segment;\n    }\n// Segments do not have dependent tasks, so once the prefetch is initiated,\n// there's nothing else for us to do (except write the server data into the\n// entry, which is handled by `fetchSegmentOnCacheMiss`).\n}\nfunction pingPPRSegmentRevalidation(now, task, currentSegment, route, routeKey, segmentKey) {\n    const revalidatingSegment = readOrCreateRevalidatingSegmentEntry(now, currentSegment);\n    switch(revalidatingSegment.status){\n        case EntryStatus.Empty:\n            // Spawn a prefetch request and upsert the segment into the cache\n            // upon completion.\n            upsertSegmentOnCompletion(task, route, segmentKey, spawnPrefetchSubtask(fetchSegmentOnCacheMiss(route, upgradeToPendingSegment(revalidatingSegment, FetchStrategy.PPR), routeKey, segmentKey)));\n            break;\n        case EntryStatus.Pending:\n            break;\n        case EntryStatus.Fulfilled:\n        case EntryStatus.Rejected:\n            break;\n        default:\n            revalidatingSegment;\n    }\n}\nfunction pingFullSegmentRevalidation(now, task, route, currentSegment, segmentKey) {\n    const revalidatingSegment = readOrCreateRevalidatingSegmentEntry(now, currentSegment);\n    if (revalidatingSegment.status === EntryStatus.Empty) {\n        // During a Full prefetch, a single dynamic request is made for all the\n        // segments that we need. So we don't initiate a request here directly. By\n        // returning a pending entry from this function, it signals to the caller\n        // that this segment should be included in the request that's sent to\n        // the server.\n        const pendingSegment = upgradeToPendingSegment(revalidatingSegment, FetchStrategy.Full);\n        upsertSegmentOnCompletion(task, route, segmentKey, waitForSegmentCacheEntry(pendingSegment));\n        return pendingSegment;\n    } else {\n        // There's already a revalidation in progress.\n        const nonEmptyRevalidatingSegment = revalidatingSegment;\n        if (nonEmptyRevalidatingSegment.fetchStrategy !== FetchStrategy.Full) {\n            // The existing revalidation was not fetched using the Full strategy.\n            // Reset it and start a new revalidation.\n            const emptySegment = resetRevalidatingSegmentEntry(nonEmptyRevalidatingSegment);\n            const pendingSegment = upgradeToPendingSegment(emptySegment, FetchStrategy.Full);\n            upsertSegmentOnCompletion(task, route, segmentKey, waitForSegmentCacheEntry(pendingSegment));\n            return pendingSegment;\n        }\n        switch(nonEmptyRevalidatingSegment.status){\n            case EntryStatus.Pending:\n                // There's already an in-progress prefetch that includes this segment.\n                return null;\n            case EntryStatus.Fulfilled:\n            case EntryStatus.Rejected:\n                // A previous revalidation attempt finished, but we chose not to replace\n                // the existing entry in the cache. Don't try again until or unless the\n                // revalidation entry expires.\n                return null;\n            default:\n                nonEmptyRevalidatingSegment;\n                return null;\n        }\n    }\n}\nconst noop = ()=>{};\nfunction upsertSegmentOnCompletion(task, route, key, promise) {\n    // Wait for a segment to finish loading, then upsert it into the cache\n    promise.then((fulfilled)=>{\n        if (fulfilled !== null) {\n            // Received new data. Attempt to replace the existing entry in the cache.\n            const keypath = getSegmentKeypathForTask(task, route, key);\n            upsertSegmentEntry(Date.now(), keypath, fulfilled);\n        }\n    }, noop);\n}\n// -----------------------------------------------------------------------------\n// The remainder of the module is a MinHeap implementation. Try not to put any\n// logic below here unless it's related to the heap algorithm. We can extract\n// this to a separate module if/when we need multiple kinds of heaps.\n// -----------------------------------------------------------------------------\nfunction compareQueuePriority(a, b) {\n    // Since the queue is a MinHeap, this should return a positive number if b is\n    // higher priority than a, and a negative number if a is higher priority\n    // than b.\n    // `priority` is an integer, where higher numbers are higher priority.\n    const priorityDiff = b.priority - a.priority;\n    if (priorityDiff !== 0) {\n        return priorityDiff;\n    }\n    // If the priority is the same, check which phase the prefetch is in — is it\n    // prefetching the route tree, or the segments? Route trees are prioritized.\n    const phaseDiff = b.phase - a.phase;\n    if (phaseDiff !== 0) {\n        return phaseDiff;\n    }\n    // Finally, check the insertion order. `sortId` is an incrementing counter\n    // assigned to prefetches. We want to process the newest prefetches first.\n    return b.sortId - a.sortId;\n}\nfunction heapPush(heap, node) {\n    const index = heap.length;\n    heap.push(node);\n    node._heapIndex = index;\n    heapSiftUp(heap, node, index);\n}\nfunction heapPeek(heap) {\n    return heap.length === 0 ? null : heap[0];\n}\nfunction heapPop(heap) {\n    if (heap.length === 0) {\n        return null;\n    }\n    const first = heap[0];\n    first._heapIndex = -1;\n    const last = heap.pop();\n    if (last !== first) {\n        heap[0] = last;\n        last._heapIndex = 0;\n        heapSiftDown(heap, last, 0);\n    }\n    return first;\n}\nfunction heapDelete(heap, node) {\n    const index = node._heapIndex;\n    if (index !== -1) {\n        node._heapIndex = -1;\n        if (heap.length !== 0) {\n            const last = heap.pop();\n            if (last !== node) {\n                heap[index] = last;\n                last._heapIndex = index;\n                heapSiftDown(heap, last, index);\n            }\n        }\n    }\n}\nfunction heapResift(heap, node) {\n    const index = node._heapIndex;\n    if (index !== -1) {\n        if (index === 0) {\n            heapSiftDown(heap, node, 0);\n        } else {\n            const parentIndex = index - 1 >>> 1;\n            const parent = heap[parentIndex];\n            if (compareQueuePriority(parent, node) > 0) {\n                // The parent is larger. Sift up.\n                heapSiftUp(heap, node, index);\n            } else {\n                // The parent is smaller (or equal). Sift down.\n                heapSiftDown(heap, node, index);\n            }\n        }\n    }\n}\nfunction heapSiftUp(heap, node, i) {\n    let index = i;\n    while(index > 0){\n        const parentIndex = index - 1 >>> 1;\n        const parent = heap[parentIndex];\n        if (compareQueuePriority(parent, node) > 0) {\n            // The parent is larger. Swap positions.\n            heap[parentIndex] = node;\n            node._heapIndex = parentIndex;\n            heap[index] = parent;\n            parent._heapIndex = index;\n            index = parentIndex;\n        } else {\n            // The parent is smaller. Exit.\n            return;\n        }\n    }\n}\nfunction heapSiftDown(heap, node, i) {\n    let index = i;\n    const length = heap.length;\n    const halfLength = length >>> 1;\n    while(index < halfLength){\n        const leftIndex = (index + 1) * 2 - 1;\n        const left = heap[leftIndex];\n        const rightIndex = leftIndex + 1;\n        const right = heap[rightIndex];\n        // If the left or right node is smaller, swap with the smaller of those.\n        if (compareQueuePriority(left, node) < 0) {\n            if (rightIndex < length && compareQueuePriority(right, left) < 0) {\n                heap[index] = right;\n                right._heapIndex = index;\n                heap[rightIndex] = node;\n                node._heapIndex = rightIndex;\n                index = rightIndex;\n            } else {\n                heap[index] = left;\n                left._heapIndex = index;\n                heap[leftIndex] = node;\n                node._heapIndex = leftIndex;\n                index = leftIndex;\n            }\n        } else if (rightIndex < length && compareQueuePriority(right, node) < 0) {\n            heap[index] = right;\n            right._heapIndex = index;\n            heap[rightIndex] = node;\n            node._heapIndex = rightIndex;\n            index = rightIndex;\n        } else {\n            // Neither child is smaller. Exit.\n            return;\n        }\n    }\n}\n\n//# sourceMappingURL=scheduler.js.map"
        }
    ]
}