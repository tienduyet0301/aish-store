{
    "sourceFile": "node_modules/next/dist/server/use-cache/use-cache-wrapper.js",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1746892894827,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1746891704042,
            "name": "cart",
            "content": "\"use strict\";\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\nObject.defineProperty(exports, \"cache\", {\n    enumerable: true,\n    get: function() {\n        return cache;\n    }\n});\nconst _serveredge = require(\"react-server-dom-webpack/server.edge\");\nconst _clientedge = require(\"react-server-dom-webpack/client.edge\");\nconst _workasyncstorageexternal = require(\"../app-render/work-async-storage.external\");\nconst _workunitasyncstorageexternal = require(\"../app-render/work-unit-async-storage.external\");\nconst _cleanasyncsnapshotexternal = require(\"../app-render/clean-async-snapshot.external\");\nconst _dynamicrenderingutils = require(\"../dynamic-rendering-utils\");\nconst _encryptionutils = require(\"../app-render/encryption-utils\");\nconst _encryption = require(\"../app-render/encryption\");\nconst _invarianterror = require(\"../../shared/lib/invariant-error\");\nconst _createerrorhandler = require(\"../app-render/create-error-handler\");\nconst _constants = require(\"./constants\");\nconst _handlers = require(\"./handlers\");\nconst _usecacheerrors = require(\"./use-cache-errors\");\nconst _dynamicrendering = require(\"../app-render/dynamic-rendering\");\nconst _searchparams = require(\"../request/search-params\");\nconst _react = /*#__PURE__*/ _interop_require_default(require(\"react\"));\nconst _lazyresult = require(\"../lib/lazy-result\");\nfunction _interop_require_default(obj) {\n    return obj && obj.__esModule ? obj : {\n        default: obj\n    };\n}\nconst isEdgeRuntime = process.env.NEXT_RUNTIME === 'edge';\nconst debug = process.env.NEXT_PRIVATE_DEBUG_CACHE ? console.debug.bind(console, 'use-cache:') : undefined;\nfunction generateCacheEntry(workStore, outerWorkUnitStore, clientReferenceManifest, encodedArguments, fn, timeoutError) {\n    // We need to run this inside a clean AsyncLocalStorage snapshot so that the cache\n    // generation cannot read anything from the context we're currently executing which\n    // might include request specific things like cookies() inside a React.cache().\n    // Note: It is important that we await at least once before this because it lets us\n    // pop out of any stack specific contexts as well - aka \"Sync\" Local Storage.\n    return (0, _cleanasyncsnapshotexternal.runInCleanSnapshot)(generateCacheEntryWithRestoredWorkStore, workStore, outerWorkUnitStore, clientReferenceManifest, encodedArguments, fn, timeoutError);\n}\nfunction generateCacheEntryWithRestoredWorkStore(workStore, outerWorkUnitStore, clientReferenceManifest, encodedArguments, fn, timeoutError) {\n    // Since we cleared the AsyncLocalStorage we need to restore the workStore.\n    // Note: We explicitly don't restore the RequestStore nor the PrerenderStore.\n    // We don't want any request specific information leaking an we don't want to create a\n    // bloated fake request mock for every cache call. So any feature that currently lives\n    // in RequestStore but should be available to Caches need to move to WorkStore.\n    // PrerenderStore is not needed inside the cache scope because the outer most one will\n    // be the one to report its result to the outer Prerender.\n    return _workasyncstorageexternal.workAsyncStorage.run(workStore, generateCacheEntryWithCacheContext, workStore, outerWorkUnitStore, clientReferenceManifest, encodedArguments, fn, timeoutError);\n}\nfunction generateCacheEntryWithCacheContext(workStore, outerWorkUnitStore, clientReferenceManifest, encodedArguments, fn, timeoutError) {\n    if (!workStore.cacheLifeProfiles) {\n        throw Object.defineProperty(new Error('cacheLifeProfiles should always be provided. This is a bug in Next.js.'), \"__NEXT_ERROR_CODE\", {\n            value: \"E294\",\n            enumerable: false,\n            configurable: true\n        });\n    }\n    const defaultCacheLife = workStore.cacheLifeProfiles['default'];\n    if (!defaultCacheLife || defaultCacheLife.revalidate == null || defaultCacheLife.expire == null || defaultCacheLife.stale == null) {\n        throw Object.defineProperty(new Error('A default cacheLife profile must always be provided. This is a bug in Next.js.'), \"__NEXT_ERROR_CODE\", {\n            value: \"E520\",\n            enumerable: false,\n            configurable: true\n        });\n    }\n    const useCacheOrRequestStore = (outerWorkUnitStore == null ? void 0 : outerWorkUnitStore.type) === 'request' || (outerWorkUnitStore == null ? void 0 : outerWorkUnitStore.type) === 'cache' ? outerWorkUnitStore : undefined;\n    // Initialize the Store for this Cache entry.\n    const cacheStore = {\n        type: 'cache',\n        phase: 'render',\n        implicitTags: outerWorkUnitStore == null ? void 0 : outerWorkUnitStore.implicitTags,\n        revalidate: defaultCacheLife.revalidate,\n        expire: defaultCacheLife.expire,\n        stale: defaultCacheLife.stale,\n        explicitRevalidate: undefined,\n        explicitExpire: undefined,\n        explicitStale: undefined,\n        tags: null,\n        hmrRefreshHash: outerWorkUnitStore && (0, _workunitasyncstorageexternal.getHmrRefreshHash)(workStore, outerWorkUnitStore),\n        isHmrRefresh: (useCacheOrRequestStore == null ? void 0 : useCacheOrRequestStore.isHmrRefresh) ?? false,\n        serverComponentsHmrCache: useCacheOrRequestStore == null ? void 0 : useCacheOrRequestStore.serverComponentsHmrCache,\n        forceRevalidate: shouldForceRevalidate(workStore, outerWorkUnitStore),\n        draftMode: outerWorkUnitStore && (0, _workunitasyncstorageexternal.getDraftModeProviderForCacheScope)(workStore, outerWorkUnitStore)\n    };\n    return _workunitasyncstorageexternal.workUnitAsyncStorage.run(cacheStore, generateCacheEntryImpl, workStore, outerWorkUnitStore, cacheStore, clientReferenceManifest, encodedArguments, fn, timeoutError);\n}\nfunction propagateCacheLifeAndTags(workUnitStore, entry) {\n    if (workUnitStore && (workUnitStore.type === 'cache' || workUnitStore.type === 'prerender' || workUnitStore.type === 'prerender-ppr' || workUnitStore.type === 'prerender-legacy')) {\n        // Propagate tags and revalidate upwards\n        const outerTags = workUnitStore.tags ?? (workUnitStore.tags = []);\n        const entryTags = entry.tags;\n        for(let i = 0; i < entryTags.length; i++){\n            const tag = entryTags[i];\n            if (!outerTags.includes(tag)) {\n                outerTags.push(tag);\n            }\n        }\n        if (workUnitStore.stale > entry.stale) {\n            workUnitStore.stale = entry.stale;\n        }\n        if (workUnitStore.revalidate > entry.revalidate) {\n            workUnitStore.revalidate = entry.revalidate;\n        }\n        if (workUnitStore.expire > entry.expire) {\n            workUnitStore.expire = entry.expire;\n        }\n    }\n}\nasync function collectResult(savedStream, workStore, outerWorkUnitStore, innerCacheStore, startTime, errors, timer) {\n    // We create a buffered stream that collects all chunks until the end to\n    // ensure that RSC has finished rendering and therefore we have collected\n    // all tags. In the future the RSC API might allow for the equivalent of\n    // the allReady Promise that exists on SSR streams.\n    //\n    // If something errored or rejected anywhere in the render, we close\n    // the stream as errored. This lets a CacheHandler choose to save the\n    // partial result up until that point for future hits for a while to avoid\n    // unnecessary retries or not to retry. We use the end of the stream for\n    // this to avoid another complicated side-channel. A receiver has to consider\n    // that the stream might also error for other reasons anyway such as losing\n    // connection.\n    const buffer = [];\n    const reader = savedStream.getReader();\n    for(let entry; !(entry = await reader.read()).done;){\n        buffer.push(entry.value);\n    }\n    let idx = 0;\n    const bufferStream = new ReadableStream({\n        pull (controller) {\n            if (workStore.invalidUsageError) {\n                controller.error(workStore.invalidUsageError);\n            } else if (idx < buffer.length) {\n                controller.enqueue(buffer[idx++]);\n            } else if (errors.length > 0) {\n                // TODO: Should we use AggregateError here?\n                controller.error(errors[0]);\n            } else {\n                controller.close();\n            }\n        }\n    });\n    const collectedTags = innerCacheStore.tags;\n    // If cacheLife() was used to set an explicit revalidate time we use that.\n    // Otherwise, we use the lowest of all inner fetch()/unstable_cache() or nested \"use cache\".\n    // If they're lower than our default.\n    const collectedRevalidate = innerCacheStore.explicitRevalidate !== undefined ? innerCacheStore.explicitRevalidate : innerCacheStore.revalidate;\n    const collectedExpire = innerCacheStore.explicitExpire !== undefined ? innerCacheStore.explicitExpire : innerCacheStore.expire;\n    const collectedStale = innerCacheStore.explicitStale !== undefined ? innerCacheStore.explicitStale : innerCacheStore.stale;\n    const entry = {\n        value: bufferStream,\n        timestamp: startTime,\n        revalidate: collectedRevalidate,\n        expire: collectedExpire,\n        stale: collectedStale,\n        tags: collectedTags === null ? [] : collectedTags\n    };\n    // Propagate tags/revalidate to the parent context.\n    propagateCacheLifeAndTags(outerWorkUnitStore, entry);\n    const cacheSignal = outerWorkUnitStore && outerWorkUnitStore.type === 'prerender' ? outerWorkUnitStore.cacheSignal : null;\n    if (cacheSignal) {\n        cacheSignal.endRead();\n    }\n    if (timer !== undefined) {\n        clearTimeout(timer);\n    }\n    return entry;\n}\nasync function generateCacheEntryImpl(workStore, outerWorkUnitStore, innerCacheStore, clientReferenceManifest, encodedArguments, fn, timeoutError) {\n    const temporaryReferences = (0, _serveredge.createTemporaryReferenceSet)();\n    const [, , args] = typeof encodedArguments === 'string' ? await (0, _serveredge.decodeReply)(encodedArguments, (0, _encryptionutils.getServerModuleMap)(), {\n        temporaryReferences\n    }) : await (0, _serveredge.decodeReplyFromAsyncIterable)({\n        async *[Symbol.asyncIterator] () {\n            for (const entry of encodedArguments){\n                yield entry;\n            }\n            // The encoded arguments might contain hanging promises. In this\n            // case we don't want to reject with \"Error: Connection closed.\",\n            // so we intentionally keep the iterable alive. This is similar to\n            // the halting trick that we do while rendering.\n            if ((outerWorkUnitStore == null ? void 0 : outerWorkUnitStore.type) === 'prerender') {\n                await new Promise((resolve)=>{\n                    if (outerWorkUnitStore.renderSignal.aborted) {\n                        resolve();\n                    } else {\n                        outerWorkUnitStore.renderSignal.addEventListener('abort', ()=>resolve(), {\n                            once: true\n                        });\n                    }\n                });\n            }\n        }\n    }, (0, _encryptionutils.getServerModuleMap)(), {\n        temporaryReferences\n    });\n    // Track the timestamp when we started computing the result.\n    const startTime = performance.timeOrigin + performance.now();\n    // Invoke the inner function to load a new result. We delay the invocation\n    // though, until React awaits the promise so that React's request store (ALS)\n    // is available when the function is invoked. This allows us, for example, to\n    // capture logs so that we can later replay them.\n    const resultPromise = (0, _lazyresult.createLazyResult)(()=>fn.apply(null, args));\n    let errors = [];\n    let timer = undefined;\n    const controller = new AbortController();\n    if ((outerWorkUnitStore == null ? void 0 : outerWorkUnitStore.type) === 'prerender') {\n        // If we're prerendering, we give you 50 seconds to fill a cache entry.\n        // Otherwise we assume you stalled on hanging input and de-opt. This needs\n        // to be lower than just the general timeout of 60 seconds.\n        timer = setTimeout(()=>{\n            controller.abort(timeoutError);\n        }, 50000);\n    }\n    const stream = (0, _serveredge.renderToReadableStream)(resultPromise, clientReferenceManifest.clientModules, {\n        environmentName: 'Cache',\n        signal: controller.signal,\n        temporaryReferences,\n        // In the \"Cache\" environment, we only need to make sure that the error\n        // digests are handled correctly. Error formatting and reporting is not\n        // necessary here; the errors are encoded in the stream, and will be\n        // reported in the \"Server\" environment.\n        onError: (error)=>{\n            const digest = (0, _createerrorhandler.getDigestForWellKnownError)(error);\n            if (digest) {\n                return digest;\n            }\n            if (process.env.NODE_ENV !== 'development') {\n                // TODO: For now we're also reporting the error here, because in\n                // production, the \"Server\" environment will only get the obfuscated\n                // error (created by the Flight Client in the cache wrapper).\n                console.error(error);\n            }\n            if (error === timeoutError) {\n                // The timeout error already aborted the whole stream. We don't need\n                // to also push this error into the `errors` array.\n                return timeoutError.digest;\n            }\n            errors.push(error);\n        }\n    });\n    const [returnStream, savedStream] = stream.tee();\n    const promiseOfCacheEntry = collectResult(savedStream, workStore, outerWorkUnitStore, innerCacheStore, startTime, errors, timer);\n    // Return the stream as we're creating it. This means that if it ends up\n    // erroring we cannot return a stale-while-error version but it allows\n    // streaming back the result earlier.\n    return [\n        returnStream,\n        promiseOfCacheEntry\n    ];\n}\nfunction cloneCacheEntry(entry) {\n    const [streamA, streamB] = entry.value.tee();\n    entry.value = streamA;\n    const clonedEntry = {\n        value: streamB,\n        timestamp: entry.timestamp,\n        revalidate: entry.revalidate,\n        expire: entry.expire,\n        stale: entry.stale,\n        tags: entry.tags\n    };\n    return [\n        entry,\n        clonedEntry\n    ];\n}\nasync function clonePendingCacheEntry(pendingCacheEntry) {\n    const entry = await pendingCacheEntry;\n    return cloneCacheEntry(entry);\n}\nasync function getNthCacheEntry(split, i) {\n    return (await split)[i];\n}\nasync function encodeFormData(formData) {\n    let result = '';\n    for (let [key, value] of formData){\n        // We don't need this key to be serializable but from a security perspective it should not be\n        // possible to generate a string that looks the same from a different structure. To ensure this\n        // we need a delimeter between fields but just using a delimeter is not enough since a string\n        // might contain that delimeter. We use the length of each field as the delimeter to avoid\n        // escaping the values.\n        result += key.length.toString(16) + ':' + key;\n        let stringValue;\n        if (typeof value === 'string') {\n            stringValue = value;\n        } else {\n            // The FormData might contain binary data that is not valid UTF-8 so this cache\n            // key may generate a UCS-2 string. Passing this to another service needs to be\n            // aware that the key might not be compatible.\n            const arrayBuffer = await value.arrayBuffer();\n            if (arrayBuffer.byteLength % 2 === 0) {\n                stringValue = String.fromCodePoint(...new Uint16Array(arrayBuffer));\n            } else {\n                stringValue = String.fromCodePoint(...new Uint16Array(arrayBuffer, 0, (arrayBuffer.byteLength - 1) / 2)) + String.fromCodePoint(new Uint8Array(arrayBuffer, arrayBuffer.byteLength - 1, 1)[0]);\n            }\n        }\n        result += stringValue.length.toString(16) + ':' + stringValue;\n    }\n    return result;\n}\nfunction createTrackedReadableStream(stream, cacheSignal) {\n    const reader = stream.getReader();\n    return new ReadableStream({\n        async pull (controller) {\n            const { done, value } = await reader.read();\n            if (done) {\n                controller.close();\n                cacheSignal.endRead();\n            } else {\n                controller.enqueue(value);\n            }\n        }\n    });\n}\nfunction cache(kind, id, boundArgsLength, originalFn) {\n    const cacheHandler = (0, _handlers.getCacheHandler)(kind);\n    if (cacheHandler === undefined) {\n        throw Object.defineProperty(new Error('Unknown cache handler: ' + kind), \"__NEXT_ERROR_CODE\", {\n            value: \"E248\",\n            enumerable: false,\n            configurable: true\n        });\n    }\n    // Capture the timeout error here to ensure a useful stack.\n    const timeoutError = new _usecacheerrors.UseCacheTimeoutError();\n    Error.captureStackTrace(timeoutError, cache);\n    const name = originalFn.name;\n    const cachedFn = {\n        [name]: async function(...args) {\n            const workStore = _workasyncstorageexternal.workAsyncStorage.getStore();\n            if (workStore === undefined) {\n                throw Object.defineProperty(new Error('\"use cache\" cannot be used outside of App Router. Expected a WorkStore.'), \"__NEXT_ERROR_CODE\", {\n                    value: \"E279\",\n                    enumerable: false,\n                    configurable: true\n                });\n            }\n            let fn = originalFn;\n            const workUnitStore = _workunitasyncstorageexternal.workUnitAsyncStorage.getStore();\n            // Get the clientReferenceManifest while we're still in the outer Context.\n            // In case getClientReferenceManifestSingleton is implemented using AsyncLocalStorage.\n            const clientReferenceManifest = (0, _encryptionutils.getClientReferenceManifestForRsc)();\n            // Because the Action ID is not yet unique per implementation of that Action we can't\n            // safely reuse the results across builds yet. In the meantime we add the buildId to the\n            // arguments as a seed to ensure they're not reused. Remove this once Action IDs hash\n            // the implementation.\n            const buildId = workStore.buildId;\n            // In dev mode, when the HMR refresh hash is set, we include it in the\n            // cache key. This ensures that cache entries are not reused when server\n            // components have been edited. This is a very coarse approach. But it's\n            // also only a temporary solution until Action IDs are unique per\n            // implementation. Remove this once Action IDs hash the implementation.\n            const hmrRefreshHash = workUnitStore && (0, _workunitasyncstorageexternal.getHmrRefreshHash)(workStore, workUnitStore);\n            const hangingInputAbortSignal = (workUnitStore == null ? void 0 : workUnitStore.type) === 'prerender' ? (0, _dynamicrendering.createHangingInputAbortSignal)(workUnitStore) : undefined;\n            // When dynamicIO is not enabled, we can not encode searchParams as\n            // hanging promises. To still avoid unused search params from making a\n            // page dynamic, we overwrite them here with a promise that resolves to an\n            // empty object, while also overwriting the to-be-invoked function for\n            // generating a cache entry with a function that creates an erroring\n            // searchParams prop before invoking the original function. This ensures\n            // that used searchParams inside of cached functions would still yield an\n            // error.\n            if (!workStore.dynamicIOEnabled && isPageComponent(args)) {\n                const [{ params, searchParams }] = args;\n                // Overwrite the props to omit $$isPageComponent.\n                args = [\n                    {\n                        params,\n                        searchParams\n                    }\n                ];\n                fn = ({\n                    [name]: async ({ params: serializedParams })=>originalFn.apply(null, [\n                            {\n                                params: serializedParams,\n                                searchParams: (0, _searchparams.makeErroringExoticSearchParamsForUseCache)(workStore)\n                            }\n                        ])\n                })[name];\n            }\n            if (boundArgsLength > 0) {\n                if (args.length === 0) {\n                    throw Object.defineProperty(new _invarianterror.InvariantError(`Expected the \"use cache\" function ${JSON.stringify(fn.name)} to receive its encrypted bound arguments as the first argument.`), \"__NEXT_ERROR_CODE\", {\n                        value: \"E524\",\n                        enumerable: false,\n                        configurable: true\n                    });\n                }\n                const encryptedBoundArgs = args.shift();\n                const boundArgs = await (0, _encryption.decryptActionBoundArgs)(id, encryptedBoundArgs);\n                if (!Array.isArray(boundArgs)) {\n                    throw Object.defineProperty(new _invarianterror.InvariantError(`Expected the bound arguments of \"use cache\" function ${JSON.stringify(fn.name)} to deserialize into an array, got ${typeof boundArgs} instead.`), \"__NEXT_ERROR_CODE\", {\n                        value: \"E581\",\n                        enumerable: false,\n                        configurable: true\n                    });\n                }\n                if (boundArgsLength !== boundArgs.length) {\n                    throw Object.defineProperty(new _invarianterror.InvariantError(`Expected the \"use cache\" function ${JSON.stringify(fn.name)} to receive ${boundArgsLength} bound arguments, got ${boundArgs.length} instead.`), \"__NEXT_ERROR_CODE\", {\n                        value: \"E559\",\n                        enumerable: false,\n                        configurable: true\n                    });\n                }\n                args.unshift(boundArgs);\n            }\n            const temporaryReferences = (0, _clientedge.createTemporaryReferenceSet)();\n            const cacheKeyParts = hmrRefreshHash ? [\n                buildId,\n                id,\n                args,\n                hmrRefreshHash\n            ] : [\n                buildId,\n                id,\n                args\n            ];\n            const encodedCacheKeyParts = await (0, _clientedge.encodeReply)(cacheKeyParts, {\n                temporaryReferences,\n                signal: hangingInputAbortSignal\n            });\n            const serializedCacheKey = typeof encodedCacheKeyParts === 'string' ? // Convert it to an ArrayBuffer if it wants to.\n            encodedCacheKeyParts : await encodeFormData(encodedCacheKeyParts);\n            let stream = undefined;\n            // Get an immutable and mutable versions of the resume data cache.\n            const prerenderResumeDataCache = workUnitStore ? (0, _workunitasyncstorageexternal.getPrerenderResumeDataCache)(workUnitStore) : null;\n            const renderResumeDataCache = workUnitStore ? (0, _workunitasyncstorageexternal.getRenderResumeDataCache)(workUnitStore) : null;\n            if (renderResumeDataCache) {\n                const cacheSignal = workUnitStore && workUnitStore.type === 'prerender' ? workUnitStore.cacheSignal : null;\n                if (cacheSignal) {\n                    cacheSignal.beginRead();\n                }\n                const cachedEntry = renderResumeDataCache.cache.get(serializedCacheKey);\n                if (cachedEntry !== undefined) {\n                    const existingEntry = await cachedEntry;\n                    propagateCacheLifeAndTags(workUnitStore, existingEntry);\n                    if (workUnitStore !== undefined && workUnitStore.type === 'prerender' && existingEntry !== undefined && (existingEntry.revalidate === 0 || existingEntry.expire < _constants.DYNAMIC_EXPIRE)) {\n                        // In a Dynamic I/O prerender, if the cache entry has revalidate: 0 or if the\n                        // expire time is under 5 minutes, then we consider this cache entry dynamic\n                        // as it's not worth generating static pages for such data. It's better to leave\n                        // a PPR hole that can be filled in dynamically with a potentially cached entry.\n                        if (cacheSignal) {\n                            cacheSignal.endRead();\n                        }\n                        return (0, _dynamicrenderingutils.makeHangingPromise)(workUnitStore.renderSignal, 'dynamic \"use cache\"');\n                    }\n                    const [streamA, streamB] = existingEntry.value.tee();\n                    existingEntry.value = streamB;\n                    if (cacheSignal) {\n                        // When we have a cacheSignal we need to block on reading the cache\n                        // entry before ending the read.\n                        stream = createTrackedReadableStream(streamA, cacheSignal);\n                    } else {\n                        stream = streamA;\n                    }\n                } else {\n                    if (cacheSignal) {\n                        cacheSignal.endRead();\n                    }\n                }\n            }\n            if (stream === undefined) {\n                var _workUnitStore_implicitTags;\n                const cacheSignal = workUnitStore && workUnitStore.type === 'prerender' ? workUnitStore.cacheSignal : null;\n                if (cacheSignal) {\n                    // Either the cache handler or the generation can be using I/O at this point.\n                    // We need to track when they start and when they complete.\n                    cacheSignal.beginRead();\n                }\n                const lazyRefreshTags = workStore.refreshTagsByCacheKind.get(kind);\n                if (lazyRefreshTags && !(0, _lazyresult.isResolvedLazyResult)(lazyRefreshTags)) {\n                    await lazyRefreshTags;\n                }\n                let entry = shouldForceRevalidate(workStore, workUnitStore) ? undefined : 'getExpiration' in cacheHandler ? await cacheHandler.get(serializedCacheKey) : // instead of checking their staleness here, as we do for modern\n                // cache handlers (see below).\n                await cacheHandler.get(serializedCacheKey, (workUnitStore == null ? void 0 : (_workUnitStore_implicitTags = workUnitStore.implicitTags) == null ? void 0 : _workUnitStore_implicitTags.tags) ?? []);\n                if (entry) {\n                    var _workUnitStore_implicitTags1;\n                    const implicitTags = (workUnitStore == null ? void 0 : (_workUnitStore_implicitTags1 = workUnitStore.implicitTags) == null ? void 0 : _workUnitStore_implicitTags1.tags) ?? [];\n                    let implicitTagsExpiration = 0;\n                    if (workUnitStore == null ? void 0 : workUnitStore.implicitTags) {\n                        const lazyExpiration = workUnitStore.implicitTags.expirationsByCacheKind.get(kind);\n                        if (lazyExpiration) {\n                            if ((0, _lazyresult.isResolvedLazyResult)(lazyExpiration)) {\n                                implicitTagsExpiration = lazyExpiration.value;\n                            } else {\n                                implicitTagsExpiration = await lazyExpiration;\n                            }\n                        }\n                    }\n                    if (shouldDiscardCacheEntry(entry, workStore, implicitTags, implicitTagsExpiration)) {\n                        debug == null ? void 0 : debug('discarding stale entry', serializedCacheKey);\n                        entry = undefined;\n                    }\n                }\n                const currentTime = performance.timeOrigin + performance.now();\n                if (workUnitStore !== undefined && workUnitStore.type === 'prerender' && entry !== undefined && (entry.revalidate === 0 || entry.expire < _constants.DYNAMIC_EXPIRE)) {\n                    // In a Dynamic I/O prerender, if the cache entry has revalidate: 0 or if the\n                    // expire time is under 5 minutes, then we consider this cache entry dynamic\n                    // as it's not worth generating static pages for such data. It's better to leave\n                    // a PPR hole that can be filled in dynamically with a potentially cached entry.\n                    if (cacheSignal) {\n                        cacheSignal.endRead();\n                    }\n                    return (0, _dynamicrenderingutils.makeHangingPromise)(workUnitStore.renderSignal, 'dynamic \"use cache\"');\n                } else if (entry === undefined || currentTime > entry.timestamp + entry.expire * 1000 || workStore.isStaticGeneration && currentTime > entry.timestamp + entry.revalidate * 1000) {\n                    // Miss. Generate a new result.\n                    // If the cache entry is stale and we're prerendering, we don't want to use the\n                    // stale entry since it would unnecessarily need to shorten the lifetime of the\n                    // prerender. We're not time constrained here so we can re-generated it now.\n                    // We need to run this inside a clean AsyncLocalStorage snapshot so that the cache\n                    // generation cannot read anything from the context we're currently executing which\n                    // might include request specific things like cookies() inside a React.cache().\n                    // Note: It is important that we await at least once before this because it lets us\n                    // pop out of any stack specific contexts as well - aka \"Sync\" Local Storage.\n                    if (entry) {\n                        if (currentTime > entry.timestamp + entry.expire * 1000) {\n                            debug == null ? void 0 : debug('entry is expired', serializedCacheKey);\n                        }\n                        if (workStore.isStaticGeneration && currentTime > entry.timestamp + entry.revalidate * 1000) {\n                            debug == null ? void 0 : debug('static generation, entry is stale', serializedCacheKey);\n                        }\n                    }\n                    const [newStream, pendingCacheEntry] = await generateCacheEntry(workStore, workUnitStore, clientReferenceManifest, encodedCacheKeyParts, fn, timeoutError);\n                    // When draft mode is enabled, we must not save the cache entry.\n                    if (!workStore.isDraftMode) {\n                        let savedCacheEntry;\n                        if (prerenderResumeDataCache) {\n                            // Create a clone that goes into the cache scope memory cache.\n                            const split = clonePendingCacheEntry(pendingCacheEntry);\n                            savedCacheEntry = getNthCacheEntry(split, 0);\n                            prerenderResumeDataCache.cache.set(serializedCacheKey, getNthCacheEntry(split, 1));\n                        } else {\n                            savedCacheEntry = pendingCacheEntry;\n                        }\n                        const promise = cacheHandler.set(serializedCacheKey, savedCacheEntry);\n                        workStore.pendingRevalidateWrites ??= [];\n                        workStore.pendingRevalidateWrites.push(promise);\n                    }\n                    stream = newStream;\n                } else {\n                    propagateCacheLifeAndTags(workUnitStore, entry);\n                    // We want to return this stream, even if it's stale.\n                    stream = entry.value;\n                    // If we have a cache scope, we need to clone the entry and set it on\n                    // the inner cache scope.\n                    if (prerenderResumeDataCache) {\n                        const [entryLeft, entryRight] = cloneCacheEntry(entry);\n                        if (cacheSignal) {\n                            stream = createTrackedReadableStream(entryLeft.value, cacheSignal);\n                        } else {\n                            stream = entryLeft.value;\n                        }\n                        prerenderResumeDataCache.cache.set(serializedCacheKey, Promise.resolve(entryRight));\n                    } else {\n                        // If we're not regenerating we need to signal that we've finished\n                        // putting the entry into the cache scope at this point. Otherwise we do\n                        // that inside generateCacheEntry.\n                        cacheSignal == null ? void 0 : cacheSignal.endRead();\n                    }\n                    if (currentTime > entry.timestamp + entry.revalidate * 1000) {\n                        // If this is stale, and we're not in a prerender (i.e. this is dynamic render),\n                        // then we should warm up the cache with a fresh revalidated entry.\n                        const [ignoredStream, pendingCacheEntry] = await generateCacheEntry(workStore, undefined, clientReferenceManifest, encodedCacheKeyParts, fn, timeoutError);\n                        let savedCacheEntry;\n                        if (prerenderResumeDataCache) {\n                            const split = clonePendingCacheEntry(pendingCacheEntry);\n                            savedCacheEntry = getNthCacheEntry(split, 0);\n                            prerenderResumeDataCache.cache.set(serializedCacheKey, getNthCacheEntry(split, 1));\n                        } else {\n                            savedCacheEntry = pendingCacheEntry;\n                        }\n                        const promise = cacheHandler.set(serializedCacheKey, savedCacheEntry);\n                        if (!workStore.pendingRevalidateWrites) {\n                            workStore.pendingRevalidateWrites = [];\n                        }\n                        workStore.pendingRevalidateWrites.push(promise);\n                        await ignoredStream.cancel();\n                    }\n                }\n            }\n            // Logs are replayed even if it's a hit - to ensure we see them on the client eventually.\n            // If we didn't then the client wouldn't see the logs if it was seeded from a prewarm that\n            // never made it to the client. However, this also means that you see logs even when the\n            // cached function isn't actually re-executed. We should instead ensure prewarms always\n            // make it to the client. Another issue is that this will cause double logging in the\n            // server terminal. Once while generating the cache entry and once when replaying it on\n            // the server, which is required to pick it up for replaying again on the client.\n            const replayConsoleLogs = true;\n            const serverConsumerManifest = {\n                // moduleLoading must be null because we don't want to trigger preloads of ClientReferences\n                // to be added to the consumer. Instead, we'll wait for any ClientReference to be emitted\n                // which themselves will handle the preloading.\n                moduleLoading: null,\n                moduleMap: isEdgeRuntime ? clientReferenceManifest.edgeRscModuleMapping : clientReferenceManifest.rscModuleMapping,\n                serverModuleMap: (0, _encryptionutils.getServerModuleMap)()\n            };\n            return (0, _clientedge.createFromReadableStream)(stream, {\n                serverConsumerManifest,\n                temporaryReferences,\n                replayConsoleLogs,\n                environmentName: 'Cache'\n            });\n        }\n    }[name];\n    return _react.default.cache(cachedFn);\n}\nfunction isPageComponent(args) {\n    if (args.length !== 2) {\n        return false;\n    }\n    const [props, ref] = args;\n    return ref === undefined && // server components receive an undefined ref arg\n    props !== null && typeof props === 'object' && props.$$isPageComponent;\n}\nfunction shouldForceRevalidate(workStore, workUnitStore) {\n    if (workStore.isOnDemandRevalidate || workStore.isDraftMode) {\n        return true;\n    }\n    if (workStore.dev && workUnitStore) {\n        if (workUnitStore.type === 'request') {\n            return workUnitStore.headers.get('cache-control') === 'no-cache';\n        }\n        if (workUnitStore.type === 'cache') {\n            return workUnitStore.forceRevalidate;\n        }\n    }\n    return false;\n}\nfunction shouldDiscardCacheEntry(entry, workStore, implicitTags, implicitTagsExpiration) {\n    // If the cache entry contains revalidated tags that the cache handler might\n    // not know about yet, we need to discard it.\n    if (entry.tags.some((tag)=>isRecentlyRevalidatedTag(tag, workStore))) {\n        return true;\n    }\n    // If the cache entry was created before any of the implicit tags were\n    // revalidated last, we also need to discard it.\n    if (entry.timestamp <= implicitTagsExpiration) {\n        debug == null ? void 0 : debug('entry was created at', entry.timestamp, 'before implicit tags were revalidated at', implicitTagsExpiration);\n        return true;\n    }\n    // Finally, if any of the implicit tags have been revalidated recently, we\n    // also need to discard the cache entry.\n    if (implicitTags.some((tag)=>isRecentlyRevalidatedTag(tag, workStore))) {\n        return true;\n    }\n    return false;\n}\nfunction isRecentlyRevalidatedTag(tag, workStore) {\n    const { previouslyRevalidatedTags, pendingRevalidatedTags } = workStore;\n    // Was the tag previously revalidated (e.g. by a redirecting server action)?\n    if (previouslyRevalidatedTags.includes(tag)) {\n        debug == null ? void 0 : debug('tag', tag, 'was previously revalidated');\n        return true;\n    }\n    // It could also have been revalidated by the currently running server action.\n    // In this case the revalidation might not have been propagated to the cache\n    // handler yet, so we read it from the pending tags in the work store.\n    if (pendingRevalidatedTags == null ? void 0 : pendingRevalidatedTags.includes(tag)) {\n        debug == null ? void 0 : debug('tag', tag, 'was just revalidated');\n        return true;\n    }\n    return false;\n}\n\n//# sourceMappingURL=use-cache-wrapper.js.map"
        }
    ]
}