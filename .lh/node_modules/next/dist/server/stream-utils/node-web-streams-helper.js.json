{
    "sourceFile": "node_modules/next/dist/server/stream-utils/node-web-streams-helper.js",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1746892891019,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1746891704042,
            "name": "cart",
            "content": "\"use strict\";\nObject.defineProperty(exports, \"__esModule\", {\n    value: true\n});\n0 && (module.exports = {\n    chainStreams: null,\n    continueDynamicHTMLResume: null,\n    continueDynamicPrerender: null,\n    continueFizzStream: null,\n    continueStaticPrerender: null,\n    createBufferedTransformStream: null,\n    createDocumentClosingStream: null,\n    createRootLayoutValidatorStream: null,\n    renderToInitialFizzStream: null,\n    streamFromBuffer: null,\n    streamFromString: null,\n    streamToBuffer: null,\n    streamToString: null\n});\nfunction _export(target, all) {\n    for(var name in all)Object.defineProperty(target, name, {\n        enumerable: true,\n        get: all[name]\n    });\n}\n_export(exports, {\n    chainStreams: function() {\n        return chainStreams;\n    },\n    continueDynamicHTMLResume: function() {\n        return continueDynamicHTMLResume;\n    },\n    continueDynamicPrerender: function() {\n        return continueDynamicPrerender;\n    },\n    continueFizzStream: function() {\n        return continueFizzStream;\n    },\n    continueStaticPrerender: function() {\n        return continueStaticPrerender;\n    },\n    createBufferedTransformStream: function() {\n        return createBufferedTransformStream;\n    },\n    createDocumentClosingStream: function() {\n        return createDocumentClosingStream;\n    },\n    createRootLayoutValidatorStream: function() {\n        return createRootLayoutValidatorStream;\n    },\n    renderToInitialFizzStream: function() {\n        return renderToInitialFizzStream;\n    },\n    streamFromBuffer: function() {\n        return streamFromBuffer;\n    },\n    streamFromString: function() {\n        return streamFromString;\n    },\n    streamToBuffer: function() {\n        return streamToBuffer;\n    },\n    streamToString: function() {\n        return streamToString;\n    }\n});\nconst _tracer = require(\"../lib/trace/tracer\");\nconst _constants = require(\"../lib/trace/constants\");\nconst _detachedpromise = require(\"../../lib/detached-promise\");\nconst _scheduler = require(\"../../lib/scheduler\");\nconst _encodedTags = require(\"./encodedTags\");\nconst _uint8arrayhelpers = require(\"./uint8array-helpers\");\nconst _constants1 = require(\"../../shared/lib/errors/constants\");\nfunction voidCatch() {\n// this catcher is designed to be used with pipeTo where we expect the underlying\n// pipe implementation to forward errors but we don't want the pipeTo promise to reject\n// and be unhandled\n}\n// We can share the same encoder instance everywhere\n// Notably we cannot do the same for TextDecoder because it is stateful\n// when handling streaming data\nconst encoder = new TextEncoder();\nfunction chainStreams(...streams) {\n    // We could encode this invariant in the arguments but current uses of this function pass\n    // use spread so it would be missed by\n    if (streams.length === 0) {\n        throw Object.defineProperty(new Error('Invariant: chainStreams requires at least one stream'), \"__NEXT_ERROR_CODE\", {\n            value: \"E437\",\n            enumerable: false,\n            configurable: true\n        });\n    }\n    // If we only have 1 stream we fast path it by returning just this stream\n    if (streams.length === 1) {\n        return streams[0];\n    }\n    const { readable, writable } = new TransformStream();\n    // We always initiate pipeTo immediately. We know we have at least 2 streams\n    // so we need to avoid closing the writable when this one finishes.\n    let promise = streams[0].pipeTo(writable, {\n        preventClose: true\n    });\n    let i = 1;\n    for(; i < streams.length - 1; i++){\n        const nextStream = streams[i];\n        promise = promise.then(()=>nextStream.pipeTo(writable, {\n                preventClose: true\n            }));\n    }\n    // We can omit the length check because we halted before the last stream and there\n    // is at least two streams so the lastStream here will always be defined\n    const lastStream = streams[i];\n    promise = promise.then(()=>lastStream.pipeTo(writable));\n    // Catch any errors from the streams and ignore them, they will be handled\n    // by whatever is consuming the readable stream.\n    promise.catch(voidCatch);\n    return readable;\n}\nfunction streamFromString(str) {\n    return new ReadableStream({\n        start (controller) {\n            controller.enqueue(encoder.encode(str));\n            controller.close();\n        }\n    });\n}\nfunction streamFromBuffer(chunk) {\n    return new ReadableStream({\n        start (controller) {\n            controller.enqueue(chunk);\n            controller.close();\n        }\n    });\n}\nasync function streamToBuffer(stream) {\n    const reader = stream.getReader();\n    const chunks = [];\n    while(true){\n        const { done, value } = await reader.read();\n        if (done) {\n            break;\n        }\n        chunks.push(value);\n    }\n    return Buffer.concat(chunks);\n}\nasync function streamToString(stream, signal) {\n    const decoder = new TextDecoder('utf-8', {\n        fatal: true\n    });\n    let string = '';\n    for await (const chunk of stream){\n        if (signal == null ? void 0 : signal.aborted) {\n            return string;\n        }\n        string += decoder.decode(chunk, {\n            stream: true\n        });\n    }\n    string += decoder.decode();\n    return string;\n}\nfunction createBufferedTransformStream() {\n    let bufferedChunks = [];\n    let bufferByteLength = 0;\n    let pending;\n    const flush = (controller)=>{\n        // If we already have a pending flush, then return early.\n        if (pending) return;\n        const detached = new _detachedpromise.DetachedPromise();\n        pending = detached;\n        (0, _scheduler.scheduleImmediate)(()=>{\n            try {\n                const chunk = new Uint8Array(bufferByteLength);\n                let copiedBytes = 0;\n                for(let i = 0; i < bufferedChunks.length; i++){\n                    const bufferedChunk = bufferedChunks[i];\n                    chunk.set(bufferedChunk, copiedBytes);\n                    copiedBytes += bufferedChunk.byteLength;\n                }\n                // We just wrote all the buffered chunks so we need to reset the bufferedChunks array\n                // and our bufferByteLength to prepare for the next round of buffered chunks\n                bufferedChunks.length = 0;\n                bufferByteLength = 0;\n                controller.enqueue(chunk);\n            } catch  {\n            // If an error occurs while enqueuing it can't be due to this\n            // transformers fault. It's likely due to the controller being\n            // errored due to the stream being cancelled.\n            } finally{\n                pending = undefined;\n                detached.resolve();\n            }\n        });\n    };\n    return new TransformStream({\n        transform (chunk, controller) {\n            // Combine the previous buffer with the new chunk.\n            bufferedChunks.push(chunk);\n            bufferByteLength += chunk.byteLength;\n            // Flush the buffer to the controller.\n            flush(controller);\n        },\n        flush () {\n            if (!pending) return;\n            return pending.promise;\n        }\n    });\n}\nfunction renderToInitialFizzStream({ ReactDOMServer, element, streamOptions }) {\n    return (0, _tracer.getTracer)().trace(_constants.AppRenderSpan.renderToReadableStream, async ()=>ReactDOMServer.renderToReadableStream(element, streamOptions));\n}\nfunction createHeadInsertionTransformStream(insert) {\n    let inserted = false;\n    // We need to track if this transform saw any bytes because if it didn't\n    // we won't want to insert any server HTML at all\n    let hasBytes = false;\n    return new TransformStream({\n        async transform (chunk, controller) {\n            hasBytes = true;\n            const insertion = await insert();\n            if (inserted) {\n                if (insertion) {\n                    const encodedInsertion = encoder.encode(insertion);\n                    controller.enqueue(encodedInsertion);\n                }\n                controller.enqueue(chunk);\n            } else {\n                // TODO (@Ethan-Arrowood): Replace the generic `indexOfUint8Array` method with something finely tuned for the subset of things actually being checked for.\n                const index = (0, _uint8arrayhelpers.indexOfUint8Array)(chunk, _encodedTags.ENCODED_TAGS.CLOSED.HEAD);\n                // In fully static rendering or non PPR rendering cases:\n                // `/head>` will always be found in the chunk in first chunk rendering.\n                if (index !== -1) {\n                    if (insertion) {\n                        const encodedInsertion = encoder.encode(insertion);\n                        // Get the total count of the bytes in the chunk and the insertion\n                        // e.g.\n                        // chunk = <head><meta charset=\"utf-8\"></head>\n                        // insertion = <script>...</script>\n                        // output = <head><meta charset=\"utf-8\"> [ <script>...</script> ] </head>\n                        const insertedHeadContent = new Uint8Array(chunk.length + encodedInsertion.length);\n                        // Append the first part of the chunk, before the head tag\n                        insertedHeadContent.set(chunk.slice(0, index));\n                        // Append the server inserted content\n                        insertedHeadContent.set(encodedInsertion, index);\n                        // Append the rest of the chunk\n                        insertedHeadContent.set(chunk.slice(index), index + encodedInsertion.length);\n                        controller.enqueue(insertedHeadContent);\n                    } else {\n                        controller.enqueue(chunk);\n                    }\n                    inserted = true;\n                } else {\n                    // This will happens in PPR rendering during next start, when the page is partially rendered.\n                    // When the page resumes, the head tag will be found in the middle of the chunk.\n                    // Where we just need to append the insertion and chunk to the current stream.\n                    // e.g.\n                    // PPR-static: <head>...</head><body> [ resume content ] </body>\n                    // PPR-resume: [ insertion ] [ rest content ]\n                    if (insertion) {\n                        controller.enqueue(encoder.encode(insertion));\n                    }\n                    controller.enqueue(chunk);\n                    inserted = true;\n                }\n            }\n        },\n        async flush (controller) {\n            // Check before closing if there's anything remaining to insert.\n            if (hasBytes) {\n                const insertion = await insert();\n                if (insertion) {\n                    controller.enqueue(encoder.encode(insertion));\n                }\n            }\n        }\n    });\n}\n// Suffix after main body content - scripts before </body>,\n// but wait for the major chunks to be enqueued.\nfunction createDeferredSuffixStream(suffix) {\n    let flushed = false;\n    let pending;\n    const flush = (controller)=>{\n        const detached = new _detachedpromise.DetachedPromise();\n        pending = detached;\n        (0, _scheduler.scheduleImmediate)(()=>{\n            try {\n                controller.enqueue(encoder.encode(suffix));\n            } catch  {\n            // If an error occurs while enqueuing it can't be due to this\n            // transformers fault. It's likely due to the controller being\n            // errored due to the stream being cancelled.\n            } finally{\n                pending = undefined;\n                detached.resolve();\n            }\n        });\n    };\n    return new TransformStream({\n        transform (chunk, controller) {\n            controller.enqueue(chunk);\n            // If we've already flushed, we're done.\n            if (flushed) return;\n            // Schedule the flush to happen.\n            flushed = true;\n            flush(controller);\n        },\n        flush (controller) {\n            if (pending) return pending.promise;\n            if (flushed) return;\n            // Flush now.\n            controller.enqueue(encoder.encode(suffix));\n        }\n    });\n}\n// Merge two streams into one. Ensure the final transform stream is closed\n// when both are finished.\nfunction createMergedTransformStream(stream) {\n    let pull = null;\n    let donePulling = false;\n    async function startPulling(controller) {\n        if (pull) {\n            return;\n        }\n        const reader = stream.getReader();\n        // NOTE: streaming flush\n        // We are buffering here for the inlined data stream because the\n        // \"shell\" stream might be chunkenized again by the underlying stream\n        // implementation, e.g. with a specific high-water mark. To ensure it's\n        // the safe timing to pipe the data stream, this extra tick is\n        // necessary.\n        // We don't start reading until we've left the current Task to ensure\n        // that it's inserted after flushing the shell. Note that this implementation\n        // might get stale if impl details of Fizz change in the future.\n        await (0, _scheduler.atLeastOneTask)();\n        try {\n            while(true){\n                const { done, value } = await reader.read();\n                if (done) {\n                    donePulling = true;\n                    return;\n                }\n                controller.enqueue(value);\n            }\n        } catch (err) {\n            controller.error(err);\n        }\n    }\n    return new TransformStream({\n        transform (chunk, controller) {\n            controller.enqueue(chunk);\n            // Start the streaming if it hasn't already been started yet.\n            if (!pull) {\n                pull = startPulling(controller);\n            }\n        },\n        flush (controller) {\n            if (donePulling) {\n                return;\n            }\n            return pull || startPulling(controller);\n        }\n    });\n}\nconst CLOSE_TAG = '</body></html>';\n/**\n * This transform stream moves the suffix to the end of the stream, so results\n * like `</body></html><script>...</script>` will be transformed to\n * `<script>...</script></body></html>`.\n */ function createMoveSuffixStream() {\n    let foundSuffix = false;\n    return new TransformStream({\n        transform (chunk, controller) {\n            if (foundSuffix) {\n                return controller.enqueue(chunk);\n            }\n            const index = (0, _uint8arrayhelpers.indexOfUint8Array)(chunk, _encodedTags.ENCODED_TAGS.CLOSED.BODY_AND_HTML);\n            if (index > -1) {\n                foundSuffix = true;\n                // If the whole chunk is the suffix, then don't write anything, it will\n                // be written in the flush.\n                if (chunk.length === _encodedTags.ENCODED_TAGS.CLOSED.BODY_AND_HTML.length) {\n                    return;\n                }\n                // Write out the part before the suffix.\n                const before = chunk.slice(0, index);\n                controller.enqueue(before);\n                // In the case where the suffix is in the middle of the chunk, we need\n                // to split the chunk into two parts.\n                if (chunk.length > _encodedTags.ENCODED_TAGS.CLOSED.BODY_AND_HTML.length + index) {\n                    // Write out the part after the suffix.\n                    const after = chunk.slice(index + _encodedTags.ENCODED_TAGS.CLOSED.BODY_AND_HTML.length);\n                    controller.enqueue(after);\n                }\n            } else {\n                controller.enqueue(chunk);\n            }\n        },\n        flush (controller) {\n            // Even if we didn't find the suffix, the HTML is not valid if we don't\n            // add it, so insert it at the end.\n            controller.enqueue(_encodedTags.ENCODED_TAGS.CLOSED.BODY_AND_HTML);\n        }\n    });\n}\nfunction createStripDocumentClosingTagsTransform() {\n    return new TransformStream({\n        transform (chunk, controller) {\n            // We rely on the assumption that chunks will never break across a code unit.\n            // This is reasonable because we currently concat all of React's output from a single\n            // flush into one chunk before streaming it forward which means the chunk will represent\n            // a single coherent utf-8 string. This is not safe to use if we change our streaming to no\n            // longer do this large buffered chunk\n            if ((0, _uint8arrayhelpers.isEquivalentUint8Arrays)(chunk, _encodedTags.ENCODED_TAGS.CLOSED.BODY_AND_HTML) || (0, _uint8arrayhelpers.isEquivalentUint8Arrays)(chunk, _encodedTags.ENCODED_TAGS.CLOSED.BODY) || (0, _uint8arrayhelpers.isEquivalentUint8Arrays)(chunk, _encodedTags.ENCODED_TAGS.CLOSED.HTML)) {\n                // the entire chunk is the closing tags; return without enqueueing anything.\n                return;\n            }\n            // We assume these tags will go at together at the end of the document and that\n            // they won't appear anywhere else in the document. This is not really a safe assumption\n            // but until we revamp our streaming infra this is a performant way to string the tags\n            chunk = (0, _uint8arrayhelpers.removeFromUint8Array)(chunk, _encodedTags.ENCODED_TAGS.CLOSED.BODY);\n            chunk = (0, _uint8arrayhelpers.removeFromUint8Array)(chunk, _encodedTags.ENCODED_TAGS.CLOSED.HTML);\n            controller.enqueue(chunk);\n        }\n    });\n}\nfunction createRootLayoutValidatorStream() {\n    let foundHtml = false;\n    let foundBody = false;\n    return new TransformStream({\n        async transform (chunk, controller) {\n            // Peek into the streamed chunk to see if the tags are present.\n            if (!foundHtml && (0, _uint8arrayhelpers.indexOfUint8Array)(chunk, _encodedTags.ENCODED_TAGS.OPENING.HTML) > -1) {\n                foundHtml = true;\n            }\n            if (!foundBody && (0, _uint8arrayhelpers.indexOfUint8Array)(chunk, _encodedTags.ENCODED_TAGS.OPENING.BODY) > -1) {\n                foundBody = true;\n            }\n            controller.enqueue(chunk);\n        },\n        flush (controller) {\n            const missingTags = [];\n            if (!foundHtml) missingTags.push('html');\n            if (!foundBody) missingTags.push('body');\n            if (!missingTags.length) return;\n            controller.enqueue(encoder.encode(`<html id=\"__next_error__\">\n            <template\n              data-next-error-message=\"Missing ${missingTags.map((c)=>`<${c}>`).join(missingTags.length > 1 ? ' and ' : '')} tags in the root layout.\\nRead more at https://nextjs.org/docs/messages/missing-root-layout-tags\"\"\n              data-next-error-digest=\"${_constants1.MISSING_ROOT_TAGS_ERROR}\"\n              data-next-error-stack=\"\"\n            ></template>\n          `));\n        }\n    });\n}\nfunction chainTransformers(readable, transformers) {\n    let stream = readable;\n    for (const transformer of transformers){\n        if (!transformer) continue;\n        stream = stream.pipeThrough(transformer);\n    }\n    return stream;\n}\nasync function continueFizzStream(renderStream, { suffix, inlinedDataStream, isStaticGeneration, getServerInsertedHTML, getServerInsertedMetadata, validateRootLayout }) {\n    // Suffix itself might contain close tags at the end, so we need to split it.\n    const suffixUnclosed = suffix ? suffix.split(CLOSE_TAG, 1)[0] : null;\n    // If we're generating static HTML and there's an `allReady` promise on the\n    // stream, we need to wait for it to resolve before continuing.\n    if (isStaticGeneration && 'allReady' in renderStream) {\n        await renderStream.allReady;\n    }\n    return chainTransformers(renderStream, [\n        // Buffer everything to avoid flushing too frequently\n        createBufferedTransformStream(),\n        // Insert generated metadata\n        createHeadInsertionTransformStream(getServerInsertedMetadata),\n        // Insert suffix content\n        suffixUnclosed != null && suffixUnclosed.length > 0 ? createDeferredSuffixStream(suffixUnclosed) : null,\n        // Insert the inlined data (Flight data, form state, etc.) stream into the HTML\n        inlinedDataStream ? createMergedTransformStream(inlinedDataStream) : null,\n        // Validate the root layout for missing html or body tags\n        validateRootLayout ? createRootLayoutValidatorStream() : null,\n        // Close tags should always be deferred to the end\n        createMoveSuffixStream(),\n        // Special head insertions\n        // TODO-APP: Insert server side html to end of head in app layout rendering, to avoid\n        // hydration errors. Remove this once it's ready to be handled by react itself.\n        createHeadInsertionTransformStream(getServerInsertedHTML)\n    ]);\n}\nasync function continueDynamicPrerender(prerenderStream, { getServerInsertedHTML, getServerInsertedMetadata }) {\n    return prerenderStream// Buffer everything to avoid flushing too frequently\n    .pipeThrough(createBufferedTransformStream()).pipeThrough(createStripDocumentClosingTagsTransform())// Insert generated tags to head\n    .pipeThrough(createHeadInsertionTransformStream(getServerInsertedHTML))// Insert generated metadata\n    .pipeThrough(createHeadInsertionTransformStream(getServerInsertedMetadata));\n}\nasync function continueStaticPrerender(prerenderStream, { inlinedDataStream, getServerInsertedHTML, getServerInsertedMetadata }) {\n    return prerenderStream// Buffer everything to avoid flushing too frequently\n    .pipeThrough(createBufferedTransformStream())// Insert generated tags to head\n    .pipeThrough(createHeadInsertionTransformStream(getServerInsertedHTML))// Insert generated metadata to head\n    .pipeThrough(createHeadInsertionTransformStream(getServerInsertedMetadata))// Insert the inlined data (Flight data, form state, etc.) stream into the HTML\n    .pipeThrough(createMergedTransformStream(inlinedDataStream))// Close tags should always be deferred to the end\n    .pipeThrough(createMoveSuffixStream());\n}\nasync function continueDynamicHTMLResume(renderStream, { inlinedDataStream, getServerInsertedHTML, getServerInsertedMetadata }) {\n    return renderStream// Buffer everything to avoid flushing too frequently\n    .pipeThrough(createBufferedTransformStream())// Insert generated tags to head\n    .pipeThrough(createHeadInsertionTransformStream(getServerInsertedHTML))// Insert generated metadata to body\n    .pipeThrough(createHeadInsertionTransformStream(getServerInsertedMetadata))// Insert the inlined data (Flight data, form state, etc.) stream into the HTML\n    .pipeThrough(createMergedTransformStream(inlinedDataStream))// Close tags should always be deferred to the end\n    .pipeThrough(createMoveSuffixStream());\n}\nfunction createDocumentClosingStream() {\n    return streamFromString(CLOSE_TAG);\n}\n\n//# sourceMappingURL=node-web-streams-helper.js.map"
        }
    ]
}