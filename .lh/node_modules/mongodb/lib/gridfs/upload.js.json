{
    "sourceFile": "node_modules/mongodb/lib/gridfs/upload.js",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1746892621384,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1746891704042,
            "name": "cart",
            "content": "\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.GridFSBucketWriteStream = void 0;\nconst stream_1 = require(\"stream\");\nconst bson_1 = require(\"../bson\");\nconst abstract_cursor_1 = require(\"../cursor/abstract_cursor\");\nconst error_1 = require(\"../error\");\nconst timeout_1 = require(\"../timeout\");\nconst utils_1 = require(\"../utils\");\nconst write_concern_1 = require(\"./../write_concern\");\n/**\n * A writable stream that enables you to write buffers to GridFS.\n *\n * Do not instantiate this class directly. Use `openUploadStream()` instead.\n * @public\n */\nclass GridFSBucketWriteStream extends stream_1.Writable {\n    /**\n     * @param bucket - Handle for this stream's corresponding bucket\n     * @param filename - The value of the 'filename' key in the files doc\n     * @param options - Optional settings.\n     * @internal\n     */\n    constructor(bucket, filename, options) {\n        super();\n        /**\n         * The document containing information about the inserted file.\n         * This property is defined _after_ the finish event has been emitted.\n         * It will remain `null` if an error occurs.\n         *\n         * @example\n         * ```ts\n         * fs.createReadStream('file.txt')\n         *   .pipe(bucket.openUploadStream('file.txt'))\n         *   .on('finish', function () {\n         *     console.log(this.gridFSFile)\n         *   })\n         * ```\n         */\n        this.gridFSFile = null;\n        options = options ?? {};\n        this.bucket = bucket;\n        this.chunks = bucket.s._chunksCollection;\n        this.filename = filename;\n        this.files = bucket.s._filesCollection;\n        this.options = options;\n        this.writeConcern = write_concern_1.WriteConcern.fromOptions(options) || bucket.s.options.writeConcern;\n        // Signals the write is all done\n        this.done = false;\n        this.id = options.id ? options.id : new bson_1.ObjectId();\n        // properly inherit the default chunksize from parent\n        this.chunkSizeBytes = options.chunkSizeBytes || this.bucket.s.options.chunkSizeBytes;\n        this.bufToStore = Buffer.alloc(this.chunkSizeBytes);\n        this.length = 0;\n        this.n = 0;\n        this.pos = 0;\n        this.state = {\n            streamEnd: false,\n            outstandingRequests: 0,\n            errored: false,\n            aborted: false\n        };\n        if (options.timeoutMS != null)\n            this.timeoutContext = new timeout_1.CSOTTimeoutContext({\n                timeoutMS: options.timeoutMS,\n                serverSelectionTimeoutMS: (0, utils_1.resolveTimeoutOptions)(this.bucket.s.db.client, {})\n                    .serverSelectionTimeoutMS\n            });\n    }\n    /**\n     * @internal\n     *\n     * The stream is considered constructed when the indexes are done being created\n     */\n    _construct(callback) {\n        if (!this.bucket.s.calledOpenUploadStream) {\n            this.bucket.s.calledOpenUploadStream = true;\n            checkIndexes(this).then(() => {\n                this.bucket.s.checkedIndexes = true;\n                this.bucket.emit('index');\n                callback();\n            }, error => {\n                if (error instanceof error_1.MongoOperationTimeoutError) {\n                    return handleError(this, error, callback);\n                }\n                (0, utils_1.squashError)(error);\n                callback();\n            });\n        }\n        else {\n            return process.nextTick(callback);\n        }\n    }\n    /**\n     * @internal\n     * Write a buffer to the stream.\n     *\n     * @param chunk - Buffer to write\n     * @param encoding - Optional encoding for the buffer\n     * @param callback - Function to call when the chunk was added to the buffer, or if the entire chunk was persisted to MongoDB if this chunk caused a flush.\n     */\n    _write(chunk, encoding, callback) {\n        doWrite(this, chunk, encoding, callback);\n    }\n    /** @internal */\n    _final(callback) {\n        if (this.state.streamEnd) {\n            return process.nextTick(callback);\n        }\n        this.state.streamEnd = true;\n        writeRemnant(this, callback);\n    }\n    /**\n     * Places this write stream into an aborted state (all future writes fail)\n     * and deletes all chunks that have already been written.\n     */\n    async abort() {\n        if (this.state.streamEnd) {\n            // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n            throw new error_1.MongoAPIError('Cannot abort a stream that has already completed');\n        }\n        if (this.state.aborted) {\n            // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n            throw new error_1.MongoAPIError('Cannot call abort() on a stream twice');\n        }\n        this.state.aborted = true;\n        const remainingTimeMS = this.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${this.timeoutContext?.timeoutMS}ms`);\n        await this.chunks.deleteMany({ files_id: this.id }, { timeoutMS: remainingTimeMS });\n    }\n}\nexports.GridFSBucketWriteStream = GridFSBucketWriteStream;\nfunction handleError(stream, error, callback) {\n    if (stream.state.errored) {\n        process.nextTick(callback);\n        return;\n    }\n    stream.state.errored = true;\n    process.nextTick(callback, error);\n}\nfunction createChunkDoc(filesId, n, data) {\n    return {\n        _id: new bson_1.ObjectId(),\n        files_id: filesId,\n        n,\n        data\n    };\n}\nasync function checkChunksIndex(stream) {\n    const index = { files_id: 1, n: 1 };\n    let remainingTimeMS;\n    remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);\n    let indexes;\n    try {\n        indexes = await stream.chunks\n            .listIndexes({\n            timeoutMode: remainingTimeMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,\n            timeoutMS: remainingTimeMS\n        })\n            .toArray();\n    }\n    catch (error) {\n        if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n            indexes = [];\n        }\n        else {\n            throw error;\n        }\n    }\n    const hasChunksIndex = !!indexes.find(index => {\n        const keys = Object.keys(index.key);\n        if (keys.length === 2 && index.key.files_id === 1 && index.key.n === 1) {\n            return true;\n        }\n        return false;\n    });\n    if (!hasChunksIndex) {\n        remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);\n        await stream.chunks.createIndex(index, {\n            ...stream.writeConcern,\n            background: true,\n            unique: true,\n            timeoutMS: remainingTimeMS\n        });\n    }\n}\nfunction checkDone(stream, callback) {\n    if (stream.done) {\n        return process.nextTick(callback);\n    }\n    if (stream.state.streamEnd && stream.state.outstandingRequests === 0 && !stream.state.errored) {\n        // Set done so we do not trigger duplicate createFilesDoc\n        stream.done = true;\n        // Create a new files doc\n        const gridFSFile = createFilesDoc(stream.id, stream.length, stream.chunkSizeBytes, stream.filename, stream.options.contentType, stream.options.aliases, stream.options.metadata);\n        if (isAborted(stream, callback)) {\n            return;\n        }\n        const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;\n        if (remainingTimeMS != null && remainingTimeMS <= 0) {\n            return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);\n        }\n        stream.files\n            .insertOne(gridFSFile, { writeConcern: stream.writeConcern, timeoutMS: remainingTimeMS })\n            .then(() => {\n            stream.gridFSFile = gridFSFile;\n            callback();\n        }, error => {\n            return handleError(stream, error, callback);\n        });\n        return;\n    }\n    process.nextTick(callback);\n}\nasync function checkIndexes(stream) {\n    let remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);\n    const doc = await stream.files.findOne({}, {\n        projection: { _id: 1 },\n        timeoutMS: remainingTimeMS\n    });\n    if (doc != null) {\n        // If at least one document exists assume the collection has the required index\n        return;\n    }\n    const index = { filename: 1, uploadDate: 1 };\n    let indexes;\n    remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);\n    const listIndexesOptions = {\n        timeoutMode: remainingTimeMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,\n        timeoutMS: remainingTimeMS\n    };\n    try {\n        indexes = await stream.files.listIndexes(listIndexesOptions).toArray();\n    }\n    catch (error) {\n        if (error instanceof error_1.MongoError && error.code === error_1.MONGODB_ERROR_CODES.NamespaceNotFound) {\n            indexes = [];\n        }\n        else {\n            throw error;\n        }\n    }\n    const hasFileIndex = !!indexes.find(index => {\n        const keys = Object.keys(index.key);\n        if (keys.length === 2 && index.key.filename === 1 && index.key.uploadDate === 1) {\n            return true;\n        }\n        return false;\n    });\n    if (!hasFileIndex) {\n        remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`);\n        await stream.files.createIndex(index, { background: false, timeoutMS: remainingTimeMS });\n    }\n    await checkChunksIndex(stream);\n}\nfunction createFilesDoc(_id, length, chunkSize, filename, contentType, aliases, metadata) {\n    const ret = {\n        _id,\n        length,\n        chunkSize,\n        uploadDate: new Date(),\n        filename\n    };\n    if (contentType) {\n        ret.contentType = contentType;\n    }\n    if (aliases) {\n        ret.aliases = aliases;\n    }\n    if (metadata) {\n        ret.metadata = metadata;\n    }\n    return ret;\n}\nfunction doWrite(stream, chunk, encoding, callback) {\n    if (isAborted(stream, callback)) {\n        return;\n    }\n    const inputBuf = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk, encoding);\n    stream.length += inputBuf.length;\n    // Input is small enough to fit in our buffer\n    if (stream.pos + inputBuf.length < stream.chunkSizeBytes) {\n        inputBuf.copy(stream.bufToStore, stream.pos);\n        stream.pos += inputBuf.length;\n        process.nextTick(callback);\n        return;\n    }\n    // Otherwise, buffer is too big for current chunk, so we need to flush\n    // to MongoDB.\n    let inputBufRemaining = inputBuf.length;\n    let spaceRemaining = stream.chunkSizeBytes - stream.pos;\n    let numToCopy = Math.min(spaceRemaining, inputBuf.length);\n    let outstandingRequests = 0;\n    while (inputBufRemaining > 0) {\n        const inputBufPos = inputBuf.length - inputBufRemaining;\n        inputBuf.copy(stream.bufToStore, stream.pos, inputBufPos, inputBufPos + numToCopy);\n        stream.pos += numToCopy;\n        spaceRemaining -= numToCopy;\n        let doc;\n        if (spaceRemaining === 0) {\n            doc = createChunkDoc(stream.id, stream.n, Buffer.from(stream.bufToStore));\n            const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;\n            if (remainingTimeMS != null && remainingTimeMS <= 0) {\n                return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);\n            }\n            ++stream.state.outstandingRequests;\n            ++outstandingRequests;\n            if (isAborted(stream, callback)) {\n                return;\n            }\n            stream.chunks\n                .insertOne(doc, { writeConcern: stream.writeConcern, timeoutMS: remainingTimeMS })\n                .then(() => {\n                --stream.state.outstandingRequests;\n                --outstandingRequests;\n                if (!outstandingRequests) {\n                    checkDone(stream, callback);\n                }\n            }, error => {\n                return handleError(stream, error, callback);\n            });\n            spaceRemaining = stream.chunkSizeBytes;\n            stream.pos = 0;\n            ++stream.n;\n        }\n        inputBufRemaining -= numToCopy;\n        numToCopy = Math.min(spaceRemaining, inputBufRemaining);\n    }\n}\nfunction writeRemnant(stream, callback) {\n    // Buffer is empty, so don't bother to insert\n    if (stream.pos === 0) {\n        return checkDone(stream, callback);\n    }\n    // Create a new buffer to make sure the buffer isn't bigger than it needs\n    // to be.\n    const remnant = Buffer.alloc(stream.pos);\n    stream.bufToStore.copy(remnant, 0, 0, stream.pos);\n    const doc = createChunkDoc(stream.id, stream.n, remnant);\n    // If the stream was aborted, do not write remnant\n    if (isAborted(stream, callback)) {\n        return;\n    }\n    const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;\n    if (remainingTimeMS != null && remainingTimeMS <= 0) {\n        return handleError(stream, new error_1.MongoOperationTimeoutError(`Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`), callback);\n    }\n    ++stream.state.outstandingRequests;\n    stream.chunks\n        .insertOne(doc, { writeConcern: stream.writeConcern, timeoutMS: remainingTimeMS })\n        .then(() => {\n        --stream.state.outstandingRequests;\n        checkDone(stream, callback);\n    }, error => {\n        return handleError(stream, error, callback);\n    });\n}\nfunction isAborted(stream, callback) {\n    if (stream.state.aborted) {\n        process.nextTick(callback, new error_1.MongoAPIError('Stream has been aborted'));\n        return true;\n    }\n    return false;\n}\n//# sourceMappingURL=upload.js.map"
        }
    ]
}