{
    "sourceFile": "node_modules/mongodb/lib/gridfs/download.js",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1746892621046,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1746891704042,
            "name": "cart",
            "content": "\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nexports.GridFSBucketReadStream = void 0;\nconst stream_1 = require(\"stream\");\nconst abstract_cursor_1 = require(\"../cursor/abstract_cursor\");\nconst error_1 = require(\"../error\");\nconst timeout_1 = require(\"../timeout\");\n/**\n * A readable stream that enables you to read buffers from GridFS.\n *\n * Do not instantiate this class directly. Use `openDownloadStream()` instead.\n * @public\n */\nclass GridFSBucketReadStream extends stream_1.Readable {\n    /**\n     * @param chunks - Handle for chunks collection\n     * @param files - Handle for files collection\n     * @param readPreference - The read preference to use\n     * @param filter - The filter to use to find the file document\n     * @internal\n     */\n    constructor(chunks, files, readPreference, filter, options) {\n        super({ emitClose: true });\n        this.s = {\n            bytesToTrim: 0,\n            bytesToSkip: 0,\n            bytesRead: 0,\n            chunks,\n            expected: 0,\n            files,\n            filter,\n            init: false,\n            expectedEnd: 0,\n            options: {\n                start: 0,\n                end: 0,\n                ...options\n            },\n            readPreference,\n            timeoutContext: options?.timeoutMS != null\n                ? new timeout_1.CSOTTimeoutContext({ timeoutMS: options.timeoutMS, serverSelectionTimeoutMS: 0 })\n                : undefined\n        };\n    }\n    /**\n     * Reads from the cursor and pushes to the stream.\n     * Private Impl, do not call directly\n     * @internal\n     */\n    _read() {\n        if (this.destroyed)\n            return;\n        waitForFile(this, () => doRead(this));\n    }\n    /**\n     * Sets the 0-based offset in bytes to start streaming from. Throws\n     * an error if this stream has entered flowing mode\n     * (e.g. if you've already called `on('data')`)\n     *\n     * @param start - 0-based offset in bytes to start streaming from\n     */\n    start(start = 0) {\n        throwIfInitialized(this);\n        this.s.options.start = start;\n        return this;\n    }\n    /**\n     * Sets the 0-based offset in bytes to start streaming from. Throws\n     * an error if this stream has entered flowing mode\n     * (e.g. if you've already called `on('data')`)\n     *\n     * @param end - Offset in bytes to stop reading at\n     */\n    end(end = 0) {\n        throwIfInitialized(this);\n        this.s.options.end = end;\n        return this;\n    }\n    /**\n     * Marks this stream as aborted (will never push another `data` event)\n     * and kills the underlying cursor. Will emit the 'end' event, and then\n     * the 'close' event once the cursor is successfully killed.\n     */\n    async abort() {\n        this.push(null);\n        this.destroy();\n        const remainingTimeMS = this.s.timeoutContext?.getRemainingTimeMSOrThrow();\n        await this.s.cursor?.close({ timeoutMS: remainingTimeMS });\n    }\n}\nexports.GridFSBucketReadStream = GridFSBucketReadStream;\n/**\n * Fires when the stream loaded the file document corresponding to the provided id.\n * @event\n */\nGridFSBucketReadStream.FILE = 'file';\nfunction throwIfInitialized(stream) {\n    if (stream.s.init) {\n        throw new error_1.MongoGridFSStreamError('Options cannot be changed after the stream is initialized');\n    }\n}\nfunction doRead(stream) {\n    if (stream.destroyed)\n        return;\n    if (!stream.s.cursor)\n        return;\n    if (!stream.s.file)\n        return;\n    const handleReadResult = (doc) => {\n        if (stream.destroyed)\n            return;\n        if (!doc) {\n            stream.push(null);\n            stream.s.cursor?.close().then(undefined, error => stream.destroy(error));\n            return;\n        }\n        if (!stream.s.file)\n            return;\n        const bytesRemaining = stream.s.file.length - stream.s.bytesRead;\n        const expectedN = stream.s.expected++;\n        const expectedLength = Math.min(stream.s.file.chunkSize, bytesRemaining);\n        if (doc.n > expectedN) {\n            return stream.destroy(new error_1.MongoGridFSChunkError(`ChunkIsMissing: Got unexpected n: ${doc.n}, expected: ${expectedN}`));\n        }\n        if (doc.n < expectedN) {\n            return stream.destroy(new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected: ${expectedN}`));\n        }\n        let buf = Buffer.isBuffer(doc.data) ? doc.data : doc.data.buffer;\n        if (buf.byteLength !== expectedLength) {\n            if (bytesRemaining <= 0) {\n                return stream.destroy(new error_1.MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected file length ${stream.s.file.length} bytes but already read ${stream.s.bytesRead} bytes`));\n            }\n            return stream.destroy(new error_1.MongoGridFSChunkError(`ChunkIsWrongSize: Got unexpected length: ${buf.byteLength}, expected: ${expectedLength}`));\n        }\n        stream.s.bytesRead += buf.byteLength;\n        if (buf.byteLength === 0) {\n            return stream.push(null);\n        }\n        let sliceStart = null;\n        let sliceEnd = null;\n        if (stream.s.bytesToSkip != null) {\n            sliceStart = stream.s.bytesToSkip;\n            stream.s.bytesToSkip = 0;\n        }\n        const atEndOfStream = expectedN === stream.s.expectedEnd - 1;\n        const bytesLeftToRead = stream.s.options.end - stream.s.bytesToSkip;\n        if (atEndOfStream && stream.s.bytesToTrim != null) {\n            sliceEnd = stream.s.file.chunkSize - stream.s.bytesToTrim;\n        }\n        else if (stream.s.options.end && bytesLeftToRead < doc.data.byteLength) {\n            sliceEnd = bytesLeftToRead;\n        }\n        if (sliceStart != null || sliceEnd != null) {\n            buf = buf.slice(sliceStart || 0, sliceEnd || buf.byteLength);\n        }\n        stream.push(buf);\n        return;\n    };\n    stream.s.cursor.next().then(handleReadResult, error => {\n        if (stream.destroyed)\n            return;\n        stream.destroy(error);\n    });\n}\nfunction init(stream) {\n    const findOneOptions = {};\n    if (stream.s.readPreference) {\n        findOneOptions.readPreference = stream.s.readPreference;\n    }\n    if (stream.s.options && stream.s.options.sort) {\n        findOneOptions.sort = stream.s.options.sort;\n    }\n    if (stream.s.options && stream.s.options.skip) {\n        findOneOptions.skip = stream.s.options.skip;\n    }\n    const handleReadResult = (doc) => {\n        if (stream.destroyed)\n            return;\n        if (!doc) {\n            const identifier = stream.s.filter._id\n                ? stream.s.filter._id.toString()\n                : stream.s.filter.filename;\n            const errmsg = `FileNotFound: file ${identifier} was not found`;\n            // TODO(NODE-3483)\n            const err = new error_1.MongoRuntimeError(errmsg);\n            err.code = 'ENOENT'; // TODO: NODE-3338 set property as part of constructor\n            return stream.destroy(err);\n        }\n        // If document is empty, kill the stream immediately and don't\n        // execute any reads\n        if (doc.length <= 0) {\n            stream.push(null);\n            return;\n        }\n        if (stream.destroyed) {\n            // If user destroys the stream before we have a cursor, wait\n            // until the query is done to say we're 'closed' because we can't\n            // cancel a query.\n            stream.destroy();\n            return;\n        }\n        try {\n            stream.s.bytesToSkip = handleStartOption(stream, doc, stream.s.options);\n        }\n        catch (error) {\n            return stream.destroy(error);\n        }\n        const filter = { files_id: doc._id };\n        // Currently (MongoDB 3.4.4) skip function does not support the index,\n        // it needs to retrieve all the documents first and then skip them. (CS-25811)\n        // As work around we use $gte on the \"n\" field.\n        if (stream.s.options && stream.s.options.start != null) {\n            const skip = Math.floor(stream.s.options.start / doc.chunkSize);\n            if (skip > 0) {\n                filter['n'] = { $gte: skip };\n            }\n        }\n        let remainingTimeMS;\n        try {\n            remainingTimeMS = stream.s.timeoutContext?.getRemainingTimeMSOrThrow(`Download timed out after ${stream.s.timeoutContext?.timeoutMS}ms`);\n        }\n        catch (error) {\n            return stream.destroy(error);\n        }\n        stream.s.cursor = stream.s.chunks\n            .find(filter, {\n            timeoutMode: stream.s.options.timeoutMS != null ? abstract_cursor_1.CursorTimeoutMode.LIFETIME : undefined,\n            timeoutMS: remainingTimeMS\n        })\n            .sort({ n: 1 });\n        if (stream.s.readPreference) {\n            stream.s.cursor.withReadPreference(stream.s.readPreference);\n        }\n        stream.s.expectedEnd = Math.ceil(doc.length / doc.chunkSize);\n        stream.s.file = doc;\n        try {\n            stream.s.bytesToTrim = handleEndOption(stream, doc, stream.s.cursor, stream.s.options);\n        }\n        catch (error) {\n            return stream.destroy(error);\n        }\n        stream.emit(GridFSBucketReadStream.FILE, doc);\n        return;\n    };\n    let remainingTimeMS;\n    try {\n        remainingTimeMS = stream.s.timeoutContext?.getRemainingTimeMSOrThrow(`Download timed out after ${stream.s.timeoutContext?.timeoutMS}ms`);\n    }\n    catch (error) {\n        if (!stream.destroyed)\n            stream.destroy(error);\n        return;\n    }\n    findOneOptions.timeoutMS = remainingTimeMS;\n    stream.s.files.findOne(stream.s.filter, findOneOptions).then(handleReadResult, error => {\n        if (stream.destroyed)\n            return;\n        stream.destroy(error);\n    });\n}\nfunction waitForFile(stream, callback) {\n    if (stream.s.file) {\n        return callback();\n    }\n    if (!stream.s.init) {\n        init(stream);\n        stream.s.init = true;\n    }\n    stream.once('file', () => {\n        callback();\n    });\n}\nfunction handleStartOption(stream, doc, options) {\n    if (options && options.start != null) {\n        if (options.start > doc.length) {\n            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be more than the length of the file (${doc.length})`);\n        }\n        if (options.start < 0) {\n            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be negative`);\n        }\n        if (options.end != null && options.end < options.start) {\n            throw new error_1.MongoInvalidArgumentError(`Stream start (${options.start}) must not be greater than stream end (${options.end})`);\n        }\n        stream.s.bytesRead = Math.floor(options.start / doc.chunkSize) * doc.chunkSize;\n        stream.s.expected = Math.floor(options.start / doc.chunkSize);\n        return options.start - stream.s.bytesRead;\n    }\n    throw new error_1.MongoInvalidArgumentError('Start option must be defined');\n}\nfunction handleEndOption(stream, doc, cursor, options) {\n    if (options && options.end != null) {\n        if (options.end > doc.length) {\n            throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be more than the length of the file (${doc.length})`);\n        }\n        if (options.start == null || options.start < 0) {\n            throw new error_1.MongoInvalidArgumentError(`Stream end (${options.end}) must not be negative`);\n        }\n        const start = options.start != null ? Math.floor(options.start / doc.chunkSize) : 0;\n        cursor.limit(Math.ceil(options.end / doc.chunkSize) - start);\n        stream.s.expectedEnd = Math.ceil(options.end / doc.chunkSize);\n        return Math.ceil(options.end / doc.chunkSize) * doc.chunkSize - options.end;\n    }\n    throw new error_1.MongoInvalidArgumentError('End option must be defined');\n}\n//# sourceMappingURL=download.js.map"
        }
    ]
}