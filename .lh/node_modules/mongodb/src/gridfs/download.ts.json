{
    "sourceFile": "node_modules/mongodb/src/gridfs/download.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1746892635112,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1746891704042,
            "name": "cart",
            "content": "import { Readable } from 'stream';\n\nimport type { Document, ObjectId } from '../bson';\nimport type { Collection } from '../collection';\nimport { CursorTimeoutMode } from '../cursor/abstract_cursor';\nimport type { FindCursor } from '../cursor/find_cursor';\nimport {\n  MongoGridFSChunkError,\n  MongoGridFSStreamError,\n  MongoInvalidArgumentError,\n  MongoRuntimeError\n} from '../error';\nimport type { FindOptions } from '../operations/find';\nimport type { ReadPreference } from '../read_preference';\nimport type { Sort } from '../sort';\nimport { CSOTTimeoutContext } from '../timeout';\nimport type { Callback } from '../utils';\nimport type { GridFSChunk } from './upload';\n\n/** @public */\nexport interface GridFSBucketReadStreamOptions {\n  sort?: Sort;\n  skip?: number;\n  /**\n   * 0-indexed non-negative byte offset from the beginning of the file\n   */\n  start?: number;\n  /**\n   * 0-indexed non-negative byte offset to the end of the file contents\n   * to be returned by the stream. `end` is non-inclusive\n   */\n  end?: number;\n  /**\n   * @experimental\n   * Specifies the time an operation will run until it throws a timeout error\n   */\n  timeoutMS?: number;\n}\n\n/** @public */\nexport interface GridFSBucketReadStreamOptionsWithRevision extends GridFSBucketReadStreamOptions {\n  /** The revision number relative to the oldest file with the given filename. 0\n   * gets you the oldest file, 1 gets you the 2nd oldest, -1 gets you the\n   * newest. */\n  revision?: number;\n}\n\n/** @public */\nexport interface GridFSFile {\n  _id: ObjectId;\n  length: number;\n  chunkSize: number;\n  filename: string;\n  metadata?: Document;\n  uploadDate: Date;\n  /** @deprecated Will be removed in the next major version. */\n  contentType?: string;\n  /** @deprecated Will be removed in the next major version. */\n  aliases?: string[];\n}\n\n/** @internal */\nexport interface GridFSBucketReadStreamPrivate {\n  /**\n   * The running total number of bytes read from the chunks collection.\n   */\n  bytesRead: number;\n  /**\n   * The number of bytes to remove from the last chunk read in the file.  This is non-zero\n   * if `end` is not equal to the length of the document and `end` is not a multiple\n   * of the chunkSize.\n   */\n  bytesToTrim: number;\n\n  /**\n   * The number of bytes to remove from the first chunk read in the file.  This is non-zero\n   * if `start` is not equal to the 0  and `start` is not a multiple\n   * of the chunkSize.\n   */\n  bytesToSkip: number;\n\n  files: Collection<GridFSFile>;\n  chunks: Collection<GridFSChunk>;\n  cursor?: FindCursor<GridFSChunk>;\n\n  /** The running total number of chunks read from the chunks collection. */\n  expected: number;\n\n  /**\n   * The filter used to search in the _files_ collection (i.e., `{ _id: <> }`)\n   * This is not the same filter used when reading chunks from the chunks collection.\n   */\n  filter: Document;\n\n  /** Indicates whether or not download has started. */\n  init: boolean;\n\n  /** The expected number of chunks to read, calculated from start, end, chunkSize and file length. */\n  expectedEnd: number;\n  file?: GridFSFile;\n  options: {\n    sort?: Sort;\n    skip?: number;\n    start: number;\n    end: number;\n    timeoutMS?: number;\n  };\n  readPreference?: ReadPreference;\n  timeoutContext?: CSOTTimeoutContext;\n}\n\n/**\n * A readable stream that enables you to read buffers from GridFS.\n *\n * Do not instantiate this class directly. Use `openDownloadStream()` instead.\n * @public\n */\nexport class GridFSBucketReadStream extends Readable {\n  /** @internal */\n  s: GridFSBucketReadStreamPrivate;\n\n  /**\n   * Fires when the stream loaded the file document corresponding to the provided id.\n   * @event\n   */\n  static readonly FILE = 'file' as const;\n\n  /**\n   * @param chunks - Handle for chunks collection\n   * @param files - Handle for files collection\n   * @param readPreference - The read preference to use\n   * @param filter - The filter to use to find the file document\n   * @internal\n   */\n  constructor(\n    chunks: Collection<GridFSChunk>,\n    files: Collection<GridFSFile>,\n    readPreference: ReadPreference | undefined,\n    filter: Document,\n    options?: GridFSBucketReadStreamOptions\n  ) {\n    super({ emitClose: true });\n    this.s = {\n      bytesToTrim: 0,\n      bytesToSkip: 0,\n      bytesRead: 0,\n      chunks,\n      expected: 0,\n      files,\n      filter,\n      init: false,\n      expectedEnd: 0,\n      options: {\n        start: 0,\n        end: 0,\n        ...options\n      },\n      readPreference,\n      timeoutContext:\n        options?.timeoutMS != null\n          ? new CSOTTimeoutContext({ timeoutMS: options.timeoutMS, serverSelectionTimeoutMS: 0 })\n          : undefined\n    };\n  }\n\n  /**\n   * Reads from the cursor and pushes to the stream.\n   * Private Impl, do not call directly\n   * @internal\n   */\n  override _read(): void {\n    if (this.destroyed) return;\n    waitForFile(this, () => doRead(this));\n  }\n\n  /**\n   * Sets the 0-based offset in bytes to start streaming from. Throws\n   * an error if this stream has entered flowing mode\n   * (e.g. if you've already called `on('data')`)\n   *\n   * @param start - 0-based offset in bytes to start streaming from\n   */\n  start(start = 0): this {\n    throwIfInitialized(this);\n    this.s.options.start = start;\n    return this;\n  }\n\n  /**\n   * Sets the 0-based offset in bytes to start streaming from. Throws\n   * an error if this stream has entered flowing mode\n   * (e.g. if you've already called `on('data')`)\n   *\n   * @param end - Offset in bytes to stop reading at\n   */\n  end(end = 0): this {\n    throwIfInitialized(this);\n    this.s.options.end = end;\n    return this;\n  }\n\n  /**\n   * Marks this stream as aborted (will never push another `data` event)\n   * and kills the underlying cursor. Will emit the 'end' event, and then\n   * the 'close' event once the cursor is successfully killed.\n   */\n  async abort(): Promise<void> {\n    this.push(null);\n    this.destroy();\n    const remainingTimeMS = this.s.timeoutContext?.getRemainingTimeMSOrThrow();\n    await this.s.cursor?.close({ timeoutMS: remainingTimeMS });\n  }\n}\n\nfunction throwIfInitialized(stream: GridFSBucketReadStream): void {\n  if (stream.s.init) {\n    throw new MongoGridFSStreamError('Options cannot be changed after the stream is initialized');\n  }\n}\n\nfunction doRead(stream: GridFSBucketReadStream): void {\n  if (stream.destroyed) return;\n  if (!stream.s.cursor) return;\n  if (!stream.s.file) return;\n\n  const handleReadResult = (doc: Document | null) => {\n    if (stream.destroyed) return;\n\n    if (!doc) {\n      stream.push(null);\n\n      stream.s.cursor?.close().then(undefined, error => stream.destroy(error));\n      return;\n    }\n\n    if (!stream.s.file) return;\n\n    const bytesRemaining = stream.s.file.length - stream.s.bytesRead;\n    const expectedN = stream.s.expected++;\n    const expectedLength = Math.min(stream.s.file.chunkSize, bytesRemaining);\n    if (doc.n > expectedN) {\n      return stream.destroy(\n        new MongoGridFSChunkError(\n          `ChunkIsMissing: Got unexpected n: ${doc.n}, expected: ${expectedN}`\n        )\n      );\n    }\n\n    if (doc.n < expectedN) {\n      return stream.destroy(\n        new MongoGridFSChunkError(`ExtraChunk: Got unexpected n: ${doc.n}, expected: ${expectedN}`)\n      );\n    }\n\n    let buf = Buffer.isBuffer(doc.data) ? doc.data : doc.data.buffer;\n\n    if (buf.byteLength !== expectedLength) {\n      if (bytesRemaining <= 0) {\n        return stream.destroy(\n          new MongoGridFSChunkError(\n            `ExtraChunk: Got unexpected n: ${doc.n}, expected file length ${stream.s.file.length} bytes but already read ${stream.s.bytesRead} bytes`\n          )\n        );\n      }\n\n      return stream.destroy(\n        new MongoGridFSChunkError(\n          `ChunkIsWrongSize: Got unexpected length: ${buf.byteLength}, expected: ${expectedLength}`\n        )\n      );\n    }\n\n    stream.s.bytesRead += buf.byteLength;\n\n    if (buf.byteLength === 0) {\n      return stream.push(null);\n    }\n\n    let sliceStart = null;\n    let sliceEnd = null;\n\n    if (stream.s.bytesToSkip != null) {\n      sliceStart = stream.s.bytesToSkip;\n      stream.s.bytesToSkip = 0;\n    }\n\n    const atEndOfStream = expectedN === stream.s.expectedEnd - 1;\n    const bytesLeftToRead = stream.s.options.end - stream.s.bytesToSkip;\n    if (atEndOfStream && stream.s.bytesToTrim != null) {\n      sliceEnd = stream.s.file.chunkSize - stream.s.bytesToTrim;\n    } else if (stream.s.options.end && bytesLeftToRead < doc.data.byteLength) {\n      sliceEnd = bytesLeftToRead;\n    }\n\n    if (sliceStart != null || sliceEnd != null) {\n      buf = buf.slice(sliceStart || 0, sliceEnd || buf.byteLength);\n    }\n\n    stream.push(buf);\n    return;\n  };\n\n  stream.s.cursor.next().then(handleReadResult, error => {\n    if (stream.destroyed) return;\n    stream.destroy(error);\n  });\n}\n\nfunction init(stream: GridFSBucketReadStream): void {\n  const findOneOptions: FindOptions = {};\n  if (stream.s.readPreference) {\n    findOneOptions.readPreference = stream.s.readPreference;\n  }\n  if (stream.s.options && stream.s.options.sort) {\n    findOneOptions.sort = stream.s.options.sort;\n  }\n  if (stream.s.options && stream.s.options.skip) {\n    findOneOptions.skip = stream.s.options.skip;\n  }\n\n  const handleReadResult = (doc: Document | null) => {\n    if (stream.destroyed) return;\n\n    if (!doc) {\n      const identifier = stream.s.filter._id\n        ? stream.s.filter._id.toString()\n        : stream.s.filter.filename;\n      const errmsg = `FileNotFound: file ${identifier} was not found`;\n      // TODO(NODE-3483)\n      const err = new MongoRuntimeError(errmsg);\n      err.code = 'ENOENT'; // TODO: NODE-3338 set property as part of constructor\n      return stream.destroy(err);\n    }\n\n    // If document is empty, kill the stream immediately and don't\n    // execute any reads\n    if (doc.length <= 0) {\n      stream.push(null);\n      return;\n    }\n\n    if (stream.destroyed) {\n      // If user destroys the stream before we have a cursor, wait\n      // until the query is done to say we're 'closed' because we can't\n      // cancel a query.\n      stream.destroy();\n      return;\n    }\n\n    try {\n      stream.s.bytesToSkip = handleStartOption(stream, doc, stream.s.options);\n    } catch (error) {\n      return stream.destroy(error);\n    }\n\n    const filter: Document = { files_id: doc._id };\n\n    // Currently (MongoDB 3.4.4) skip function does not support the index,\n    // it needs to retrieve all the documents first and then skip them. (CS-25811)\n    // As work around we use $gte on the \"n\" field.\n    if (stream.s.options && stream.s.options.start != null) {\n      const skip = Math.floor(stream.s.options.start / doc.chunkSize);\n      if (skip > 0) {\n        filter['n'] = { $gte: skip };\n      }\n    }\n\n    let remainingTimeMS: number | undefined;\n    try {\n      remainingTimeMS = stream.s.timeoutContext?.getRemainingTimeMSOrThrow(\n        `Download timed out after ${stream.s.timeoutContext?.timeoutMS}ms`\n      );\n    } catch (error) {\n      return stream.destroy(error);\n    }\n\n    stream.s.cursor = stream.s.chunks\n      .find(filter, {\n        timeoutMode: stream.s.options.timeoutMS != null ? CursorTimeoutMode.LIFETIME : undefined,\n        timeoutMS: remainingTimeMS\n      })\n      .sort({ n: 1 });\n\n    if (stream.s.readPreference) {\n      stream.s.cursor.withReadPreference(stream.s.readPreference);\n    }\n\n    stream.s.expectedEnd = Math.ceil(doc.length / doc.chunkSize);\n    stream.s.file = doc as GridFSFile;\n\n    try {\n      stream.s.bytesToTrim = handleEndOption(stream, doc, stream.s.cursor, stream.s.options);\n    } catch (error) {\n      return stream.destroy(error);\n    }\n\n    stream.emit(GridFSBucketReadStream.FILE, doc);\n    return;\n  };\n\n  let remainingTimeMS: number | undefined;\n  try {\n    remainingTimeMS = stream.s.timeoutContext?.getRemainingTimeMSOrThrow(\n      `Download timed out after ${stream.s.timeoutContext?.timeoutMS}ms`\n    );\n  } catch (error) {\n    if (!stream.destroyed) stream.destroy(error);\n    return;\n  }\n\n  findOneOptions.timeoutMS = remainingTimeMS;\n\n  stream.s.files.findOne(stream.s.filter, findOneOptions).then(handleReadResult, error => {\n    if (stream.destroyed) return;\n    stream.destroy(error);\n  });\n}\n\nfunction waitForFile(stream: GridFSBucketReadStream, callback: Callback): void {\n  if (stream.s.file) {\n    return callback();\n  }\n\n  if (!stream.s.init) {\n    init(stream);\n    stream.s.init = true;\n  }\n\n  stream.once('file', () => {\n    callback();\n  });\n}\n\nfunction handleStartOption(\n  stream: GridFSBucketReadStream,\n  doc: Document,\n  options: GridFSBucketReadStreamOptions\n): number {\n  if (options && options.start != null) {\n    if (options.start > doc.length) {\n      throw new MongoInvalidArgumentError(\n        `Stream start (${options.start}) must not be more than the length of the file (${doc.length})`\n      );\n    }\n    if (options.start < 0) {\n      throw new MongoInvalidArgumentError(`Stream start (${options.start}) must not be negative`);\n    }\n    if (options.end != null && options.end < options.start) {\n      throw new MongoInvalidArgumentError(\n        `Stream start (${options.start}) must not be greater than stream end (${options.end})`\n      );\n    }\n\n    stream.s.bytesRead = Math.floor(options.start / doc.chunkSize) * doc.chunkSize;\n    stream.s.expected = Math.floor(options.start / doc.chunkSize);\n\n    return options.start - stream.s.bytesRead;\n  }\n  throw new MongoInvalidArgumentError('Start option must be defined');\n}\n\nfunction handleEndOption(\n  stream: GridFSBucketReadStream,\n  doc: Document,\n  cursor: FindCursor<GridFSChunk>,\n  options: GridFSBucketReadStreamOptions\n) {\n  if (options && options.end != null) {\n    if (options.end > doc.length) {\n      throw new MongoInvalidArgumentError(\n        `Stream end (${options.end}) must not be more than the length of the file (${doc.length})`\n      );\n    }\n    if (options.start == null || options.start < 0) {\n      throw new MongoInvalidArgumentError(`Stream end (${options.end}) must not be negative`);\n    }\n\n    const start = options.start != null ? Math.floor(options.start / doc.chunkSize) : 0;\n\n    cursor.limit(Math.ceil(options.end / doc.chunkSize) - start);\n\n    stream.s.expectedEnd = Math.ceil(options.end / doc.chunkSize);\n\n    return Math.ceil(options.end / doc.chunkSize) * doc.chunkSize - options.end;\n  }\n  throw new MongoInvalidArgumentError('End option must be defined');\n}\n"
        }
    ]
}