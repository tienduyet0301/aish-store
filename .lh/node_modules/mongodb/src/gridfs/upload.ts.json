{
    "sourceFile": "node_modules/mongodb/src/gridfs/upload.ts",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1746892635169,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1746891704042,
            "name": "cart",
            "content": "import { Writable } from 'stream';\n\nimport { type Document, ObjectId } from '../bson';\nimport type { Collection } from '../collection';\nimport { CursorTimeoutMode } from '../cursor/abstract_cursor';\nimport {\n  MongoAPIError,\n  MONGODB_ERROR_CODES,\n  MongoError,\n  MongoOperationTimeoutError\n} from '../error';\nimport { CSOTTimeoutContext } from '../timeout';\nimport { type Callback, resolveTimeoutOptions, squashError } from '../utils';\nimport type { WriteConcernOptions } from '../write_concern';\nimport { WriteConcern } from './../write_concern';\nimport type { GridFSFile } from './download';\nimport type { GridFSBucket } from './index';\n\n/** @public */\nexport interface GridFSChunk {\n  _id: ObjectId;\n  files_id: ObjectId;\n  n: number;\n  data: Buffer | Uint8Array;\n}\n\n/** @public */\nexport interface GridFSBucketWriteStreamOptions extends WriteConcernOptions {\n  /** Overwrite this bucket's chunkSizeBytes for this file */\n  chunkSizeBytes?: number;\n  /** Custom file id for the GridFS file. */\n  id?: ObjectId;\n  /** Object to store in the file document's `metadata` field */\n  metadata?: Document;\n  /**\n   * String to store in the file document's `contentType` field.\n   * @deprecated Will be removed in the next major version. Add a contentType field to the metadata document instead.\n   */\n  contentType?: string;\n  /**\n   * Array of strings to store in the file document's `aliases` field.\n   * @deprecated Will be removed in the next major version. Add an aliases field to the metadata document instead.\n   */\n  aliases?: string[];\n  /**\n   * @experimental\n   * Specifies the time an operation will run until it throws a timeout error\n   */\n  timeoutMS?: number;\n}\n\n/**\n * A writable stream that enables you to write buffers to GridFS.\n *\n * Do not instantiate this class directly. Use `openUploadStream()` instead.\n * @public\n */\nexport class GridFSBucketWriteStream extends Writable {\n  bucket: GridFSBucket;\n  /** A Collection instance where the file's chunks are stored */\n  chunks: Collection<GridFSChunk>;\n  /** A Collection instance where the file's GridFSFile document is stored */\n  files: Collection<GridFSFile>;\n  /** The name of the file */\n  filename: string;\n  /** Options controlling the metadata inserted along with the file */\n  options: GridFSBucketWriteStreamOptions;\n  /** Indicates the stream is finished uploading */\n  done: boolean;\n  /** The ObjectId used for the `_id` field on the GridFSFile document */\n  id: ObjectId;\n  /** The number of bytes that each chunk will be limited to */\n  chunkSizeBytes: number;\n  /** Space used to store a chunk currently being inserted */\n  bufToStore: Buffer;\n  /** Accumulates the number of bytes inserted as the stream uploads chunks */\n  length: number;\n  /** Accumulates the number of chunks inserted as the stream uploads file contents */\n  n: number;\n  /** Tracks the current offset into the buffered bytes being uploaded */\n  pos: number;\n  /** Contains a number of properties indicating the current state of the stream */\n  state: {\n    /** If set the stream has ended */\n    streamEnd: boolean;\n    /** Indicates the number of chunks that still need to be inserted to exhaust the current buffered data */\n    outstandingRequests: number;\n    /** If set an error occurred during insertion */\n    errored: boolean;\n    /** If set the stream was intentionally aborted */\n    aborted: boolean;\n  };\n  /** The write concern setting to be used with every insert operation */\n  writeConcern?: WriteConcern;\n  /**\n   * The document containing information about the inserted file.\n   * This property is defined _after_ the finish event has been emitted.\n   * It will remain `null` if an error occurs.\n   *\n   * @example\n   * ```ts\n   * fs.createReadStream('file.txt')\n   *   .pipe(bucket.openUploadStream('file.txt'))\n   *   .on('finish', function () {\n   *     console.log(this.gridFSFile)\n   *   })\n   * ```\n   */\n  gridFSFile: GridFSFile | null = null;\n  /** @internal */\n  timeoutContext?: CSOTTimeoutContext;\n\n  /**\n   * @param bucket - Handle for this stream's corresponding bucket\n   * @param filename - The value of the 'filename' key in the files doc\n   * @param options - Optional settings.\n   * @internal\n   */\n  constructor(bucket: GridFSBucket, filename: string, options?: GridFSBucketWriteStreamOptions) {\n    super();\n\n    options = options ?? {};\n    this.bucket = bucket;\n    this.chunks = bucket.s._chunksCollection;\n    this.filename = filename;\n    this.files = bucket.s._filesCollection;\n    this.options = options;\n    this.writeConcern = WriteConcern.fromOptions(options) || bucket.s.options.writeConcern;\n    // Signals the write is all done\n    this.done = false;\n\n    this.id = options.id ? options.id : new ObjectId();\n    // properly inherit the default chunksize from parent\n    this.chunkSizeBytes = options.chunkSizeBytes || this.bucket.s.options.chunkSizeBytes;\n    this.bufToStore = Buffer.alloc(this.chunkSizeBytes);\n    this.length = 0;\n    this.n = 0;\n    this.pos = 0;\n    this.state = {\n      streamEnd: false,\n      outstandingRequests: 0,\n      errored: false,\n      aborted: false\n    };\n\n    if (options.timeoutMS != null)\n      this.timeoutContext = new CSOTTimeoutContext({\n        timeoutMS: options.timeoutMS,\n        serverSelectionTimeoutMS: resolveTimeoutOptions(this.bucket.s.db.client, {})\n          .serverSelectionTimeoutMS\n      });\n  }\n\n  /**\n   * @internal\n   *\n   * The stream is considered constructed when the indexes are done being created\n   */\n  override _construct(callback: (error?: Error | null) => void): void {\n    if (!this.bucket.s.calledOpenUploadStream) {\n      this.bucket.s.calledOpenUploadStream = true;\n\n      checkIndexes(this).then(\n        () => {\n          this.bucket.s.checkedIndexes = true;\n          this.bucket.emit('index');\n          callback();\n        },\n        error => {\n          if (error instanceof MongoOperationTimeoutError) {\n            return handleError(this, error, callback);\n          }\n          squashError(error);\n          callback();\n        }\n      );\n    } else {\n      return process.nextTick(callback);\n    }\n  }\n\n  /**\n   * @internal\n   * Write a buffer to the stream.\n   *\n   * @param chunk - Buffer to write\n   * @param encoding - Optional encoding for the buffer\n   * @param callback - Function to call when the chunk was added to the buffer, or if the entire chunk was persisted to MongoDB if this chunk caused a flush.\n   */\n  override _write(\n    chunk: Buffer | string,\n    encoding: BufferEncoding,\n    callback: Callback<void>\n  ): void {\n    doWrite(this, chunk, encoding, callback);\n  }\n\n  /** @internal */\n  override _final(callback: (error?: Error | null) => void): void {\n    if (this.state.streamEnd) {\n      return process.nextTick(callback);\n    }\n    this.state.streamEnd = true;\n    writeRemnant(this, callback);\n  }\n\n  /**\n   * Places this write stream into an aborted state (all future writes fail)\n   * and deletes all chunks that have already been written.\n   */\n  async abort(): Promise<void> {\n    if (this.state.streamEnd) {\n      // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n      throw new MongoAPIError('Cannot abort a stream that has already completed');\n    }\n\n    if (this.state.aborted) {\n      // TODO(NODE-3485): Replace with MongoGridFSStreamClosed\n      throw new MongoAPIError('Cannot call abort() on a stream twice');\n    }\n\n    this.state.aborted = true;\n    const remainingTimeMS = this.timeoutContext?.getRemainingTimeMSOrThrow(\n      `Upload timed out after ${this.timeoutContext?.timeoutMS}ms`\n    );\n\n    await this.chunks.deleteMany({ files_id: this.id }, { timeoutMS: remainingTimeMS });\n  }\n}\n\nfunction handleError(stream: GridFSBucketWriteStream, error: Error, callback: Callback): void {\n  if (stream.state.errored) {\n    process.nextTick(callback);\n    return;\n  }\n  stream.state.errored = true;\n  process.nextTick(callback, error);\n}\n\nfunction createChunkDoc(filesId: ObjectId, n: number, data: Buffer): GridFSChunk {\n  return {\n    _id: new ObjectId(),\n    files_id: filesId,\n    n,\n    data\n  };\n}\n\nasync function checkChunksIndex(stream: GridFSBucketWriteStream): Promise<void> {\n  const index = { files_id: 1, n: 1 };\n\n  let remainingTimeMS;\n  remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(\n    `Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`\n  );\n\n  let indexes;\n  try {\n    indexes = await stream.chunks\n      .listIndexes({\n        timeoutMode: remainingTimeMS != null ? CursorTimeoutMode.LIFETIME : undefined,\n        timeoutMS: remainingTimeMS\n      })\n      .toArray();\n  } catch (error) {\n    if (error instanceof MongoError && error.code === MONGODB_ERROR_CODES.NamespaceNotFound) {\n      indexes = [];\n    } else {\n      throw error;\n    }\n  }\n\n  const hasChunksIndex = !!indexes.find(index => {\n    const keys = Object.keys(index.key);\n    if (keys.length === 2 && index.key.files_id === 1 && index.key.n === 1) {\n      return true;\n    }\n    return false;\n  });\n\n  if (!hasChunksIndex) {\n    remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(\n      `Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`\n    );\n    await stream.chunks.createIndex(index, {\n      ...stream.writeConcern,\n      background: true,\n      unique: true,\n      timeoutMS: remainingTimeMS\n    });\n  }\n}\n\nfunction checkDone(stream: GridFSBucketWriteStream, callback: Callback): void {\n  if (stream.done) {\n    return process.nextTick(callback);\n  }\n\n  if (stream.state.streamEnd && stream.state.outstandingRequests === 0 && !stream.state.errored) {\n    // Set done so we do not trigger duplicate createFilesDoc\n    stream.done = true;\n    // Create a new files doc\n    const gridFSFile = createFilesDoc(\n      stream.id,\n      stream.length,\n      stream.chunkSizeBytes,\n      stream.filename,\n      stream.options.contentType,\n      stream.options.aliases,\n      stream.options.metadata\n    );\n\n    if (isAborted(stream, callback)) {\n      return;\n    }\n\n    const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;\n    if (remainingTimeMS != null && remainingTimeMS <= 0) {\n      return handleError(\n        stream,\n        new MongoOperationTimeoutError(\n          `Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`\n        ),\n        callback\n      );\n    }\n\n    stream.files\n      .insertOne(gridFSFile, { writeConcern: stream.writeConcern, timeoutMS: remainingTimeMS })\n      .then(\n        () => {\n          stream.gridFSFile = gridFSFile;\n          callback();\n        },\n        error => {\n          return handleError(stream, error, callback);\n        }\n      );\n    return;\n  }\n\n  process.nextTick(callback);\n}\n\nasync function checkIndexes(stream: GridFSBucketWriteStream): Promise<void> {\n  let remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(\n    `Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`\n  );\n  const doc = await stream.files.findOne(\n    {},\n    {\n      projection: { _id: 1 },\n      timeoutMS: remainingTimeMS\n    }\n  );\n  if (doc != null) {\n    // If at least one document exists assume the collection has the required index\n    return;\n  }\n\n  const index = { filename: 1, uploadDate: 1 };\n\n  let indexes;\n  remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(\n    `Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`\n  );\n  const listIndexesOptions = {\n    timeoutMode: remainingTimeMS != null ? CursorTimeoutMode.LIFETIME : undefined,\n    timeoutMS: remainingTimeMS\n  };\n  try {\n    indexes = await stream.files.listIndexes(listIndexesOptions).toArray();\n  } catch (error) {\n    if (error instanceof MongoError && error.code === MONGODB_ERROR_CODES.NamespaceNotFound) {\n      indexes = [];\n    } else {\n      throw error;\n    }\n  }\n\n  const hasFileIndex = !!indexes.find(index => {\n    const keys = Object.keys(index.key);\n    if (keys.length === 2 && index.key.filename === 1 && index.key.uploadDate === 1) {\n      return true;\n    }\n    return false;\n  });\n\n  if (!hasFileIndex) {\n    remainingTimeMS = stream.timeoutContext?.getRemainingTimeMSOrThrow(\n      `Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`\n    );\n\n    await stream.files.createIndex(index, { background: false, timeoutMS: remainingTimeMS });\n  }\n\n  await checkChunksIndex(stream);\n}\n\nfunction createFilesDoc(\n  _id: ObjectId,\n  length: number,\n  chunkSize: number,\n  filename: string,\n  contentType?: string,\n  aliases?: string[],\n  metadata?: Document\n): GridFSFile {\n  const ret: GridFSFile = {\n    _id,\n    length,\n    chunkSize,\n    uploadDate: new Date(),\n    filename\n  };\n\n  if (contentType) {\n    ret.contentType = contentType;\n  }\n\n  if (aliases) {\n    ret.aliases = aliases;\n  }\n\n  if (metadata) {\n    ret.metadata = metadata;\n  }\n\n  return ret;\n}\n\nfunction doWrite(\n  stream: GridFSBucketWriteStream,\n  chunk: Buffer | string,\n  encoding: BufferEncoding,\n  callback: Callback<void>\n): void {\n  if (isAborted(stream, callback)) {\n    return;\n  }\n\n  const inputBuf = Buffer.isBuffer(chunk) ? chunk : Buffer.from(chunk, encoding);\n\n  stream.length += inputBuf.length;\n\n  // Input is small enough to fit in our buffer\n  if (stream.pos + inputBuf.length < stream.chunkSizeBytes) {\n    inputBuf.copy(stream.bufToStore, stream.pos);\n    stream.pos += inputBuf.length;\n    process.nextTick(callback);\n    return;\n  }\n\n  // Otherwise, buffer is too big for current chunk, so we need to flush\n  // to MongoDB.\n  let inputBufRemaining = inputBuf.length;\n  let spaceRemaining: number = stream.chunkSizeBytes - stream.pos;\n  let numToCopy = Math.min(spaceRemaining, inputBuf.length);\n  let outstandingRequests = 0;\n  while (inputBufRemaining > 0) {\n    const inputBufPos = inputBuf.length - inputBufRemaining;\n    inputBuf.copy(stream.bufToStore, stream.pos, inputBufPos, inputBufPos + numToCopy);\n    stream.pos += numToCopy;\n    spaceRemaining -= numToCopy;\n    let doc: GridFSChunk;\n    if (spaceRemaining === 0) {\n      doc = createChunkDoc(stream.id, stream.n, Buffer.from(stream.bufToStore));\n\n      const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;\n      if (remainingTimeMS != null && remainingTimeMS <= 0) {\n        return handleError(\n          stream,\n          new MongoOperationTimeoutError(\n            `Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`\n          ),\n          callback\n        );\n      }\n\n      ++stream.state.outstandingRequests;\n      ++outstandingRequests;\n\n      if (isAborted(stream, callback)) {\n        return;\n      }\n\n      stream.chunks\n        .insertOne(doc, { writeConcern: stream.writeConcern, timeoutMS: remainingTimeMS })\n        .then(\n          () => {\n            --stream.state.outstandingRequests;\n            --outstandingRequests;\n\n            if (!outstandingRequests) {\n              checkDone(stream, callback);\n            }\n          },\n          error => {\n            return handleError(stream, error, callback);\n          }\n        );\n\n      spaceRemaining = stream.chunkSizeBytes;\n      stream.pos = 0;\n      ++stream.n;\n    }\n    inputBufRemaining -= numToCopy;\n    numToCopy = Math.min(spaceRemaining, inputBufRemaining);\n  }\n}\n\nfunction writeRemnant(stream: GridFSBucketWriteStream, callback: Callback): void {\n  // Buffer is empty, so don't bother to insert\n  if (stream.pos === 0) {\n    return checkDone(stream, callback);\n  }\n\n  // Create a new buffer to make sure the buffer isn't bigger than it needs\n  // to be.\n  const remnant = Buffer.alloc(stream.pos);\n  stream.bufToStore.copy(remnant, 0, 0, stream.pos);\n  const doc = createChunkDoc(stream.id, stream.n, remnant);\n\n  // If the stream was aborted, do not write remnant\n  if (isAborted(stream, callback)) {\n    return;\n  }\n\n  const remainingTimeMS = stream.timeoutContext?.remainingTimeMS;\n  if (remainingTimeMS != null && remainingTimeMS <= 0) {\n    return handleError(\n      stream,\n      new MongoOperationTimeoutError(\n        `Upload timed out after ${stream.timeoutContext?.timeoutMS}ms`\n      ),\n      callback\n    );\n  }\n  ++stream.state.outstandingRequests;\n  stream.chunks\n    .insertOne(doc, { writeConcern: stream.writeConcern, timeoutMS: remainingTimeMS })\n    .then(\n      () => {\n        --stream.state.outstandingRequests;\n        checkDone(stream, callback);\n      },\n      error => {\n        return handleError(stream, error, callback);\n      }\n    );\n}\n\nfunction isAborted(stream: GridFSBucketWriteStream, callback: Callback<void>): boolean {\n  if (stream.state.aborted) {\n    process.nextTick(callback, new MongoAPIError('Stream has been aborted'));\n    return true;\n  }\n  return false;\n}\n"
        }
    ]
}